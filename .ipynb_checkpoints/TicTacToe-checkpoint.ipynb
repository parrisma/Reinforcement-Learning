{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import math\n",
    "import unittest\n",
    "import operator\n",
    "import numbers\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "\n",
    "    # There are 5812 legal board states that can be reached before there is a winner\n",
    "    # http://brianshourd.com/posts/2012-11-06-tilt-number-of-tic-tac-toe-boards.html\n",
    "\n",
    "    __bad_move_game_is_over = -1\n",
    "    __bad_move_action_already_played = -2\n",
    "    __bad_move_no_consecutive_plays = -3\n",
    "    __play = float(0)  # reward for playing an action\n",
    "    __draw = float(2)  # reward for playing to end but no one wins\n",
    "    __win = float(1)  # reward for winning a game\n",
    "    __loss = float(-2)  # reward (penalty) for losing a game\n",
    "    __rewards = {\"Play\": 0, \"Draw\": 2, \"Win\": 1, \"Loss\": -2}\n",
    "    __no_player = np.nan  # id of a non existent player i.e. used to record id of player that has not played\n",
    "    __win_mask = np.full((1, 3), 3, np.int8)\n",
    "    __actions = {1: (0, 0), 2: (0, 1), 3: (0, 2), 4: (1, 0), 5: (1, 1), 6: (1, 2), 7: (2, 0), 8: (2, 1), 9: (2, 2)}\n",
    "    player_X = 1  # numerical value of player X on the board\n",
    "    player_O = -1  # numerical value of player O on the board\n",
    "    empty_cell = np.nan  # value of a free action space on board\n",
    "    asStr = True\n",
    "\n",
    "    #\n",
    "    # Return game to initial state, where no one has played\n",
    "    # and the board contains no moves.\n",
    "    #\n",
    "    def reset(self):\n",
    "        self.__board = np.full((3, 3), np.nan)\n",
    "        self.__last_board = np.full((3, 3), np.nan)\n",
    "        self.__game_over = False\n",
    "        self.__game_drawn = False\n",
    "        self.__player = TicTacToe.__no_player\n",
    "        self.__last_player = TicTacToe.__no_player\n",
    "\n",
    "    #\n",
    "    # Constructor has no arguments as it just sets the game\n",
    "    # to an intial up-played set-up\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__board = np.full((3, 3), np.nan)\n",
    "        self.__last_board = np.full((3, 3), np.nan)\n",
    "        self.__game_over = False\n",
    "        self.__game_drawn = False\n",
    "        self.__player = TicTacToe.__no_player\n",
    "        self.__last_player = TicTacToe.__no_player\n",
    "\n",
    "    #\n",
    "    # Return a displayable version of the entire game.\n",
    "    #\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"Game Over: \" + str(self.__game_over) + \"\\n\"\n",
    "        s += \"Player :\" + TicTacToe.__player_to_str(self.__player) + \"\\n\"\n",
    "        s += \"Current Board : \\n\" + str(self.__board) + \"\\n\"\n",
    "        s += \"Prev Player :\" + TicTacToe.__player_to_str(self.__last_player) + \"\\n\"\n",
    "        s += \"Prev Current Board : \\n\" + str(self.__last_board) + \"\\n\"\n",
    "        return s\n",
    "\n",
    "    #\n",
    "    # return player as string \"X\" or \"O\"\n",
    "    #\n",
    "    @classmethod\n",
    "    def __player_to_str(cls, player):\n",
    "        if player == TicTacToe.player_X: return \"X\"\n",
    "        if player == TicTacToe.player_O: return \"O\"\n",
    "        return \"?\"\n",
    "\n",
    "    #\n",
    "    # return player as integer\n",
    "    #\n",
    "    @classmethod\n",
    "    def player_to_int(cls, player):\n",
    "        if np.isnan(player):\n",
    "            return 0\n",
    "        return int(player)\n",
    "\n",
    "    #\n",
    "    # Return the number of possible actions as a list of integers.\n",
    "    #\n",
    "    @classmethod\n",
    "    def num_actions(cls):\n",
    "        return len(TicTacToe.__actions)\n",
    "\n",
    "    #\n",
    "    # Return the maximum number of moves per game.\n",
    "    #\n",
    "    @classmethod\n",
    "    def max_moves_per_game(cls):\n",
    "        return len(TicTacToe.__actions)\n",
    "\n",
    "    #\n",
    "    # Return the actions as a list of integers.\n",
    "    #\n",
    "    @classmethod\n",
    "    def actions(cls):\n",
    "        return list(map(lambda a: int(a), list(TicTacToe.__actions.keys())))\n",
    "\n",
    "    #\n",
    "    # Return the board index (i,j) of a given action\n",
    "    #\n",
    "    @classmethod\n",
    "    def board_index(cls, action):\n",
    "        return TicTacToe.__actions[action]\n",
    "\n",
    "    #\n",
    "    # Return rewards as dictionary where key is name of reward\n",
    "    # and the value is the reward\n",
    "    #\n",
    "    @classmethod\n",
    "    def rewards(cls):\n",
    "        return TicTacToe.__rewards\n",
    "\n",
    "    #\n",
    "    # Assume the move has been validated by move method\n",
    "    # Make a copy of board before move is made and the last player\n",
    "    #\n",
    "    def __make_move(self, action, player):\n",
    "        self.__last_board = np.copy(self.__board)\n",
    "        self.__last_player = self.__player\n",
    "        self.__player = player\n",
    "        self.__board[TicTacToe.board_index(action)] = player\n",
    "        return\n",
    "\n",
    "    #\n",
    "    # Has a player already moved using the given action.\n",
    "    #\n",
    "    def __invalid_move(self, action):\n",
    "        return not np.isnan(self.__board[TicTacToe.board_index(action)])\n",
    "\n",
    "    #\n",
    "    # If the proposed action is a valid move and the game is not\n",
    "    # over. Make the given move (action) on behalf of the given\n",
    "    # player and update the game status.\n",
    "    #\n",
    "    # return the rawards (Player who took move, Observer)\n",
    "    #\n",
    "    def move(self, action, player):\n",
    "        if TicTacToe.game_won(self.__board): return TicTacToe.__bad_move_game_is_over\n",
    "        if self.__invalid_move(action): return TicTacToe.__bad_move_action_already_played\n",
    "        if player == self.__player: return TicTacToe.__bad_move_no_consecutive_plays\n",
    "\n",
    "        self.__make_move(action, player)\n",
    "\n",
    "        if (TicTacToe.game_won(self.__board)):\n",
    "            self.__game_over = True\n",
    "            self.__game_drawn = False\n",
    "            return np.array([TicTacToe.__win, TicTacToe.__loss])\n",
    "\n",
    "        if (not TicTacToe.moves_left_to_take(self.__board)):\n",
    "            self.__game_over = True\n",
    "            self.__game_drawn = True\n",
    "            return np.array([TicTacToe.__draw, TicTacToe.__draw])\n",
    "\n",
    "        return np.array([TicTacToe.__play, 0])\n",
    "\n",
    "    #\n",
    "    # Return (flattened) Game Ended, Last Player, Last Board, Player, Board\n",
    "    #\n",
    "    def detailed_state(self):\n",
    "        flattened_state = []\n",
    "        if (self.__game_over):\n",
    "            flattened_state.append(1)\n",
    "        else:\n",
    "            flattened_state.append(0)\n",
    "        flattened_state.append(self.__last_player)\n",
    "        flattened_state.append(self.__player)\n",
    "        for itm in np.reshape(self.__last_board, 9).tolist(): flattened_state.append(itm)\n",
    "        for itm in np.reshape(self.__board, 9).tolist(): flattened_state.append(itm)\n",
    "\n",
    "        return flattened_state\n",
    "\n",
    "    #\n",
    "    # Show return the current board contents\n",
    "    #\n",
    "    def board(self):\n",
    "        return self.__board\n",
    "\n",
    "    #\n",
    "    # Any row, column or diagonal with all player X or player O. If a\n",
    "    # player is given then it answers has that specific player won\n",
    "    #\n",
    "    @classmethod\n",
    "    def game_won(cls, bd, plyr=None):\n",
    "\n",
    "        if not plyr is None: bd = (bd == plyr) * 1\n",
    "\n",
    "        rows = np.abs(np.sum(bd, axis=1))\n",
    "        cols = np.abs(np.sum(bd, axis=0))\n",
    "        diag_lr = np.abs(np.sum(bd.diagonal()))\n",
    "        diag_rl = np.abs(np.sum(np.rot90(bd).diagonal()))\n",
    "\n",
    "        if np.sum(rows == 3) > 0:\n",
    "            return True\n",
    "        if np.sum(cols == 3) > 0:\n",
    "            return True\n",
    "        if not np.isnan(diag_lr):\n",
    "            if ((np.mod(diag_lr, 3)) == 0) and diag_lr > 0:\n",
    "                return True\n",
    "        if not np.isnan(diag_rl):\n",
    "            if ((np.mod(diag_rl, 3)) == 0) and diag_rl > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    #\n",
    "    # Are there any remaining moves to be taken >\n",
    "    #\n",
    "    @classmethod\n",
    "    def moves_left_to_take(cls, bd):\n",
    "        return bd[np.isnan(bd)].size > 0\n",
    "\n",
    "    #\n",
    "    # Board is in a gamne over state, with a winner or a draw\n",
    "    #\n",
    "    @classmethod\n",
    "    def board_game_over(cls, board):\n",
    "        return (TicTacToe.game_won(board) or not TicTacToe.moves_left_to_take(board))\n",
    "\n",
    "    #\n",
    "    # Is the game over ?\n",
    "    #\n",
    "    def game_over(self):\n",
    "        return TicTacToe.board_game_over(self.__board)\n",
    "\n",
    "    #\n",
    "    # Return which player goes next given the current player\n",
    "    #\n",
    "    @staticmethod\n",
    "    def other_player(current_player):\n",
    "        if (current_player == TicTacToe.player_O):\n",
    "            return TicTacToe.player_X\n",
    "        else:\n",
    "            return TicTacToe.player_O\n",
    "\n",
    "    #\n",
    "    # What moves are valid for the given board\n",
    "    #\n",
    "    @classmethod\n",
    "    def valid_moves(cls, board):\n",
    "        vm = np.zeros(TicTacToe.num_actions())\n",
    "        best_action = None\n",
    "        for actn in TicTacToe.actions():\n",
    "            if (board[TicTacToe.board_index(actn)] == 0):\n",
    "                vm[int(actn) - 1] = True\n",
    "            else:\n",
    "                vm[int(actn) - 1] = False\n",
    "        return vm\n",
    "\n",
    "    #\n",
    "    # What moves are valid given for board or if not\n",
    "    # for the current game board.\n",
    "    #\n",
    "    def what_are_valid_moves(self):\n",
    "        return TicTacToe.valid_moves(self.__board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlayTicTacToe:\n",
    "\n",
    "    __learning_rate_0 = 0.05\n",
    "    __learning_rate_decay = 0.001\n",
    "    __discount_factor = .8\n",
    "    __Q = {}  # learning spans game sessions.\n",
    "\n",
    "    #\n",
    "    # Constructor has no arguments as it just sets the game\n",
    "    # to an intial up-played set-up\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__game = TicTacToe()\n",
    "\n",
    "    #\n",
    "    # Return the current game\n",
    "    #\n",
    "    def game(self):\n",
    "        return self.__game\n",
    "\n",
    "    #\n",
    "    # Set leared state to given QValues.\n",
    "    #\n",
    "    def transfer_learning(self, QV):\n",
    "        self.__Q = QV\n",
    "        print(\"Learned Games:\" + str(len(self.__Q)))\n",
    "\n",
    "    #\n",
    "    # The learned Q Values for a given state if they exist\n",
    "    #\n",
    "    def Q_Vals_for_state(self, state):\n",
    "        if (state in self.__Q):\n",
    "            return (self.__Q[state])\n",
    "        else:\n",
    "            return (None)\n",
    "\n",
    "    #\n",
    "    # Expose the current class instance learning in terms of Q Values.\n",
    "    #\n",
    "    def Q_Vals(self):\n",
    "        return (self.__Q)\n",
    "\n",
    "    #\n",
    "    # Forget learning\n",
    "    #\n",
    "    def forget_learning(self):\n",
    "        self.__Q = {}\n",
    "\n",
    "    #\n",
    "    # Add states to Q Value dictionary if not present\n",
    "    #\n",
    "    def add_states_if_missing(self, s1):\n",
    "        if s1 not in self.__Q:\n",
    "            self.__Q[s1] = np.full(TicTacToe.num_actions(), np.nan)\n",
    "\n",
    "    #\n",
    "    # Return the learning rate paramaters\n",
    "    #\n",
    "    @classmethod\n",
    "    def learning_rate_params(cls):\n",
    "        return cls.__learning_rate_0, cls.__learning_rate_decay, cls.__discount_factor\n",
    "\n",
    "    #\n",
    "    # Return the learnign rate based on number of learnings to date\n",
    "    #\n",
    "    @classmethod\n",
    "    def q_learning_rate(cls, n):\n",
    "        return cls.__learning_rate_0 / (1 + (n * cls.__learning_rate_decay))\n",
    "\n",
    "    #\n",
    "    # Keep Score of players as Q Val Trains.\n",
    "    #\n",
    "    @classmethod\n",
    "    def __init_score(cls):\n",
    "        score = dict()\n",
    "        score[TicTacToe.player_X] = {}\n",
    "        score[TicTacToe.player_O] = {}\n",
    "        for rn, rv in TicTacToe.rewards().items():\n",
    "            score[TicTacToe.player_X][rv] = 0\n",
    "            score[TicTacToe.player_O][rv] = 0\n",
    "        return score\n",
    "\n",
    "    @classmethod\n",
    "    def __keep_score(cls, score, plyr, reward):\n",
    "        (score[plyr])[reward[0]] += 1\n",
    "        (score[TicTacToe.other_player(plyr)])[reward[1]] += 1\n",
    "        return\n",
    "\n",
    "    #\n",
    "    # Return the State, Action Key from the perspective of given player\n",
    "    #\n",
    "    @classmethod\n",
    "    def state(cls, player, board):\n",
    "        sa = \"\"\n",
    "        sa += str(player)\n",
    "        for cell in np.reshape(board, 9).tolist(): sa += str(TicTacToe.player_to_int(cell))\n",
    "        return sa\n",
    "\n",
    "    #\n",
    "    # Given q values for move to a given state select the\n",
    "    # as to maximise players gain or if not gain to be had\n",
    "    # minimise opponents gain\n",
    "    #\n",
    "    @classmethod\n",
    "    def best_outcome(cls, q):\n",
    "        q = q[np.isnan(q) == False]\n",
    "        if len(q) > 0:\n",
    "            stand_to_lose = np.max((q >= 0) * q)\n",
    "            stand_to_win = -np.min((q < 0) * q)\n",
    "            return max(stand_to_win, stand_to_lose)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    #\n",
    "    # The best move is the highest gain or the least loss\n",
    "    #\n",
    "    @classmethod\n",
    "    def best_move(cls, q):\n",
    "        # Best Win\n",
    "        gain = np.max((q >= 0) * q)\n",
    "        # Least Loss\n",
    "        qnz = q[np.where(q != 0)]\n",
    "        idx = max((-qnz == min(-qnz)) * np.nonzero(q)[0])\n",
    "        loss = q[idx]\n",
    "        if (gain > abs(loss)):\n",
    "            return gain\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    #\n",
    "    # Return zero float if given number is \"nan\"\n",
    "    #\n",
    "    @classmethod\n",
    "    def zero_if_nan(cls,n):\n",
    "        if np.isnan(n):\n",
    "            return 0\n",
    "        else:\n",
    "            return n\n",
    "\n",
    "    # Run simulation to estimate Q values for state, action pairs. Random exploration policy\n",
    "    # which should be tractable with approx 6K valid board states.\n",
    "    #\n",
    "    def train_Q_values(self, num_episodes, canned_moves):\n",
    "\n",
    "        # Simulation defaults.\n",
    "        learning_rate0, learning_rate_decay, discount_rate = PlayTicTacToe.learning_rate_params()\n",
    "\n",
    "        # Initalization\n",
    "        reward = 0\n",
    "        sim = 0\n",
    "        game_step = 0\n",
    "        score = PlayTicTacToe.__init_score()\n",
    "\n",
    "        # Iterate over and play\n",
    "        while (sim < num_episodes):\n",
    "            self.__game.reset()\n",
    "            plyr = None\n",
    "            prev_plyr = None\n",
    "            s = None\n",
    "            mv = None\n",
    "            prev_mv = None\n",
    "            prev_s = None\n",
    "            mv = None\n",
    "\n",
    "            game_step = 0\n",
    "            while (not self.__game.game_over()):\n",
    "\n",
    "                prev_mv = mv\n",
    "                print(str(sim) + \" : \" + str(game_step))\n",
    "                plyr, mv = (canned_moves[sim])[game_step]\n",
    "                prev_plyr = TicTacToe.other_player(plyr)\n",
    "                prev_s = s\n",
    "\n",
    "                s = PlayTicTacToe.state(plyr, self.__game.board())\n",
    "                reward = self.__game.move(mv, plyr)\n",
    "                learning_rate = PlayTicTacToe.q_learning_rate(len(self.__Q))\n",
    "\n",
    "                self.add_states_if_missing(s)\n",
    "\n",
    "                # Update Q Values for both players based on last play reward.\n",
    "                (self.__Q[s])[mv - 1] = (learning_rate * (self.zero_if_nan(self.__Q[s][mv - 1]))) + ((1 - learning_rate) * reward[0])\n",
    "                if (not prev_s is None):\n",
    "                    (self.__Q[prev_s])[prev_mv - 1] -= (discount_rate * self.best_outcome(self.__Q[s]))\n",
    "                game_step += 1\n",
    "            sim += 1\n",
    "            game_step = 0\n",
    "\n",
    "            PlayTicTacToe.__keep_score(score, plyr, reward)\n",
    "\n",
    "            if ((sim % 1000) == 0) or (sim == num_episodes):\n",
    "                smX = \"Player X : \" + str(sim) + \" : \"\n",
    "                smO = \"Player O : \" + str(sim) + \" : \"\n",
    "                for rn, rv in TicTacToe.rewards().items():\n",
    "                    smX += rn + \" : \" + str(round(((score[TicTacToe.player_X])[rv] / sim) * 100, 0)) + \"% \"\n",
    "                    smO += rn + \" : \" + str(round(((score[TicTacToe.player_O])[rv] / sim) * 100, 0)) + \"% \"\n",
    "                print(smX)\n",
    "                print(smO)\n",
    "        return self.__Q\n",
    "\n",
    "    #\n",
    "    # Run simulation to estimate Q values for state, action pairs. Random exploration policy\n",
    "    # which should be tractable with approx 6K valid board states.\n",
    "    #\n",
    "    def train_Q_values_R(self, num_simulations):\n",
    "\n",
    "        learning_rate0, learning_rate_decay, discount_rate = PlayTicTacToe.learning_rate_params()\n",
    "\n",
    "        reward = 0\n",
    "        sim = 0\n",
    "        game_step = 0\n",
    "        score = PlayTicTacToe.__init_score()\n",
    "\n",
    "        while (sim < num_simulations):\n",
    "            self.__game.reset()\n",
    "            plyr = None\n",
    "            s = None\n",
    "            mv = None\n",
    "            prev_mv = None\n",
    "            prev_s = None\n",
    "\n",
    "            plyr = (TicTacToe.player_X, TicTacToe.player_O)[randint(0, 1)]  # Random player to start\n",
    "\n",
    "            mv = None\n",
    "            while (not self.__game.game_over()):\n",
    "\n",
    "                prev_mv = mv\n",
    "                mv = self.random_move()\n",
    "\n",
    "                prev_s = s\n",
    "                s = PlayTicTacToe.state(plyr, self.__game.board())\n",
    "                reward = self.__game.move(mv, plyr)\n",
    "                learning_rate = PlayTicTacToe.q_learning_rate(len(self.__Q))\n",
    "\n",
    "                self.add_states_if_missing(s)\n",
    "\n",
    "                # Update Q Values for both players based on last play reward.\n",
    "                (self.__Q[s])[mv - 1] = (learning_rate * (self.zero_if_nan(self.__Q[s][mv - 1]))) + ((1 - learning_rate) * reward[0])\n",
    "                if (not prev_s is None):\n",
    "                    (self.__Q[prev_s])[prev_mv - 1] -= (discount_rate * self.best_outcome(self.__Q[s]))\n",
    "\n",
    "                plyr = TicTacToe.other_player(plyr)\n",
    "                game_step += 1\n",
    "            sim += 1\n",
    "            game_step = 0\n",
    "\n",
    "            PlayTicTacToe.__keep_score(score, plyr, reward)\n",
    "\n",
    "            if ((sim % 1000) == 0) or (sim == num_simulations):\n",
    "                smX = \"Player X : \" + str(sim) + \" : \"\n",
    "                smO = \"Player O : \" + str(sim) + \" : \"\n",
    "                for rn, rv in TicTacToe.rewards().items():\n",
    "                    smX += rn + \" : \" + str(round(((score[TicTacToe.player_X])[rv] / sim) * 100, 0)) + \"% \"\n",
    "                    smO += rn + \" : \" + str(round(((score[TicTacToe.player_O])[rv] / sim) * 100, 0)) + \"% \"\n",
    "                print(smX)\n",
    "                print(smO)\n",
    "        return self.__Q\n",
    "\n",
    "    #\n",
    "    # Return a random action (move) that is still left\n",
    "    # to make\n",
    "    #\n",
    "    def random_move(self):\n",
    "        valid_moves = None\n",
    "        random_action = None\n",
    "\n",
    "        valid_moves = np.isnan(self.__game.board().reshape(9))*self.__game.actions()\n",
    "        valid_moves = valid_moves[np.where(valid_moves>0)]\n",
    "\n",
    "        num_poss_moves = len(valid_moves)\n",
    "        if num_poss_moves > 0:\n",
    "            random_action = valid_moves[randint(0, num_poss_moves - 1)]\n",
    "            return random_action\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    #\n",
    "    # Given current state and lerned Q Values (if any) suggest\n",
    "    # the move that is expected to yield the highest reward.\n",
    "    #\n",
    "    def informed_move(self, st, rnd):\n",
    "        # What moves are possible at this stage\n",
    "        valid_moves = self.__game.what_are_valid_moves()\n",
    "\n",
    "        # Are there any moves ?\n",
    "        if (np.sum(valid_moves * np.full(9, 1)) == 0):\n",
    "            return None\n",
    "\n",
    "        best_action = None\n",
    "        if (not rnd):\n",
    "            # Is there info learned for this state ?\n",
    "            informed_actions = self.Q_Vals_for_state(st)\n",
    "            if not informed_actions is None:\n",
    "                informed_actions *= valid_moves\n",
    "                best_action = PlayTicTacToe.best_move(informed_actions)\n",
    "                if (best_action > 0):\n",
    "                    informed_actions = np.arange(1, TicTacToe.num_actions() + 1, 1)[\n",
    "                        np.where(informed_actions == best_action)]\n",
    "                    best_action = informed_actions[randint(0, informed_actions.size - 1)]\n",
    "                else:\n",
    "                    best_action = None\n",
    "\n",
    "        # If we found a good action then return that\n",
    "        # else pick a random action\n",
    "        if best_action == None:\n",
    "            actions = valid_moves * np.arange(1, TicTacToe.num_actions() + 1, 1)\n",
    "            actions = actions[np.where(actions > 0)]\n",
    "            best_action = actions[randint(0, actions.size - 1)]\n",
    "\n",
    "        return int(best_action)\n",
    "        #\n",
    "\n",
    "    # Play an automated game between a random player and an\n",
    "    # informed player.\n",
    "    # Return the move sequence for the entire game as s string.\n",
    "    #\n",
    "    def play(self):\n",
    "        self.__game.reset()\n",
    "        plyr = (TicTacToe.player_X, TicTacToe.player_O)[randint(0, 1)]  # Chose random player to start\n",
    "        mv = None\n",
    "        profile = \"\"\n",
    "        while (not self.__game.game_over()):\n",
    "            st = PlayTicTacToe.state(plyr, self.__game.board())\n",
    "            QV = self.Q_Vals_for_state(st)\n",
    "            mx = np.max(self.Q_Vals_for_state(st))\n",
    "            if (plyr == TicTacToe.player_X):\n",
    "                mv = self.informed_move(st, False)  # Informed Player\n",
    "            else:\n",
    "                mv = self.informed_move(st, True)  # Random Player\n",
    "            self.__game.move(mv, plyr)\n",
    "            profile += str(plyr) + \":\" + str(mv) + \"~\"\n",
    "            plyr = TicTacToe.other_player(plyr)\n",
    "        return profile\n",
    "\n",
    "    #\n",
    "    # Add the game profile to the given game dictionary and\n",
    "    # up the count for the number of times that games was played\n",
    "    #\n",
    "    @classmethod\n",
    "    def record_game_stats(cls, D, profile):\n",
    "        if profile in D:\n",
    "            D[profile] += 1\n",
    "        else:\n",
    "            D[profile] = 1\n",
    "        return\n",
    "\n",
    "    def play_many(self, num):\n",
    "        informed_wins = 0\n",
    "        random_wins = 0\n",
    "        draws = 0\n",
    "        I = {}\n",
    "        R = {}\n",
    "        D = {}\n",
    "        G = {}\n",
    "        profile = \"\"\n",
    "        for x in range(0, num):\n",
    "            profile = self.play()\n",
    "            if profile not in G: G[profile] = \"\"\n",
    "            if self.__game.game_won(self.__game.board(), TicTacToe.player_X):\n",
    "                informed_wins += 1\n",
    "                PlayTicTacToe.record_game_stats(I, profile)\n",
    "            else:\n",
    "                if self.__game.game_won(self.__game.board(), TicTacToe.player_O):\n",
    "                    random_wins += 1\n",
    "                    PlayTicTacToe.record_game_stats(R, profile)\n",
    "                else:\n",
    "                    PlayTicTacToe.record_game_stats(D, profile)\n",
    "                    draws += 1\n",
    "            if (x % 100) == 0: print (str(x))\n",
    "        print(\"Informed :\" + str(informed_wins) + \" : \" + str(round((informed_wins / num) * 100, 0)))\n",
    "        print(\"Random :\" + str(random_wins) + \" : \" + str(round((random_wins / num) * 100, 0)))\n",
    "        print(\"Draw :\" + str(draws) + \" : \" + str(round((draws / num) * 100, 0)))\n",
    "        print(\"Diff Games :\" + str(len(G)))\n",
    "        return (I, R, D)\n",
    "\n",
    "    #\n",
    "    # move_str is of form \"1:8~-1:1~1:6~-1:3~1:9~-1:2~\"\n",
    "    # plyr:action~.. repreat players must be alternate X,O (1,-1..)\n",
    "    # there is always a trailing ~\n",
    "\n",
    "    #\n",
    "    # Convert a game profile string returned from play method\n",
    "    # into an array that can be passed as a canned-move to\n",
    "    # training. (Q learn)\n",
    "    #\n",
    "    @classmethod\n",
    "    def move_str_to_array(cls, moves_as_str):\n",
    "        mvd = {}\n",
    "        mvc = 0\n",
    "        mvs = moves_as_str.split('~')\n",
    "        for mv in mvs:\n",
    "            if (len(mv) > 0):\n",
    "                pl, ps = mv.split(\":\")\n",
    "                mvd[mvc] = (int(pl), int(ps))\n",
    "            mvc += 1\n",
    "        return mvd\n",
    "\n",
    "    #\n",
    "    # Convert a game profile string returned from play method\n",
    "    # into an array that can be passed as a canned-move to\n",
    "    # training. (Q learn)\n",
    "    #\n",
    "    @classmethod\n",
    "    def move_str_to_board(cls, moves_as_str):\n",
    "        mvd = {}\n",
    "        mvc = 0\n",
    "        mvs = moves_as_str.split('~')\n",
    "        bd = np.zeros((3 * 3), np.int8)\n",
    "        for mv in mvs:\n",
    "            if (len(mv) > 0):\n",
    "                pl, ps = mv.split(\":\")\n",
    "                bd[int(ps) - 1] = int(pl)\n",
    "            mvc += 1\n",
    "        return np.reshape(bd, (3, 3))\n",
    "\n",
    "    #\n",
    "    # Convert a dictionary of game profiles returned from play_many\n",
    "    # to a dictionary of canned moves that can be passed to training (Q Learn)\n",
    "    #\n",
    "    @classmethod\n",
    "    def moves_to_dict(cls, D):\n",
    "        MD = {}\n",
    "        i = 0\n",
    "        for mvss, cnt in D.items():\n",
    "            MD[i] = PlayTicTacToe.move_str_to_array(mvss)\n",
    "            i += 1\n",
    "        return MD\n",
    "\n",
    "    #\n",
    "    # All possible endings. Generate moves str's for all the possible endings of the\n",
    "    # game from the perspective of the prev player.\n",
    "    #\n",
    "    # The given moves must be the moves of a valid game that played to either win/draw\n",
    "    # including the last move that won/drew the game.\n",
    "    #\n",
    "    @classmethod\n",
    "    def all_possible_endings(cls, moves_as_str, exclude_current_ending=True):\n",
    "        APE = {}\n",
    "        mvs = PlayTicTacToe.move_str_to_array(moves_as_str)\n",
    "\n",
    "        terminal_move = mvs[len(mvs) - 1]  # The move that won, drew\n",
    "        last_move = mvs[len(mvs) - 2]  # the move we will replace with all other options\n",
    "\n",
    "        t_plyr = terminal_move[0]\n",
    "        t_actn = terminal_move[1]\n",
    "\n",
    "        l_plyr = last_move[0]\n",
    "        l_actn = last_move[1]\n",
    "\n",
    "        base_game = \"~\".join(moves_as_str.split(\"~\")[:-3])  # less Trailing ~ + terminal & last move\n",
    "        bd = PlayTicTacToe.move_str_to_board(base_game)\n",
    "        vmvs = TicTacToe.valid_moves(bd)\n",
    "        a = 1\n",
    "        for vm in vmvs:\n",
    "            poss_end = base_game\n",
    "            if (vm):\n",
    "                if (a != t_actn):  # don't include the terminal action as we will add that back on.\n",
    "                    if (not (exclude_current_ending and a == l_actn)):\n",
    "                        poss_end += \"~\" + str(l_plyr) + \":\" + str(a)\n",
    "                        poss_end += \"~\" + str(t_plyr) + \":\" + str(t_actn) + \"~\"\n",
    "                        APE[poss_end] = 0\n",
    "            a += 1\n",
    "\n",
    "        return (APE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.22765039e-06\n"
     ]
    }
   ],
   "source": [
    "qv=np.array((-1.12074287e-03  ,-8.15610660e-01,  -8.15281066e-01 , -8.13555037e-01,  -8.20507102e-01 , -2.22765039e-06 , -8.24740712e-01 , -8.15325657e-01,   -8.18110732e-01))\n",
    "print(PlayTicTacToe.best_move(qv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pqv(qv):\n",
    "    print(\"[ %3.5f %3.5f %3.5f :: %3.5f %3.5f %3.5f :: %3.5f %3.5f %3.5f ]\" % (qv[0],qv[1],qv[2],qv[3],qv[4],qv[5],qv[6],qv[7],qv[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "play = PlayTicTacToe()\n",
    "play.forget_learning()\n",
    "print(play.Q_Vals())\n",
    "#play.transfer_learning(QV)\n",
    "#print(len(play.Q_Vals()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-1:1~1:4~-1:9~1:7~-1:5~': 0, '-1:1~1:4~-1:9~1:3~-1:5~': 0, '-1:1~1:4~-1:9~1:6~-1:5~': 0, '-1:1~1:4~-1:9~1:8~-1:5~': 0, '-1:1~1:4~-1:9~1:2~-1:5~': 0}\n",
      "0 : 0\n",
      "0 : 1\n",
      "0 : 2\n",
      "0 : 3\n",
      "0 : 4\n",
      "1 : 0\n",
      "1 : 1\n",
      "1 : 2\n",
      "1 : 3\n",
      "1 : 4\n",
      "2 : 0\n",
      "2 : 1\n",
      "2 : 2\n",
      "2 : 3\n",
      "2 : 4\n",
      "3 : 0\n",
      "3 : 1\n",
      "3 : 2\n",
      "3 : 3\n",
      "3 : 4\n",
      "4 : 0\n",
      "4 : 1\n",
      "4 : 2\n",
      "4 : 3\n",
      "4 : 4\n",
      "Player X : 5 : Loss : 100.0% Win : 0.0% Draw : 0.0% Play : 0.0% \n",
      "Player O : 5 : Loss : 0.0% Win : 100.0% Draw : 0.0% Play : 0.0% \n"
     ]
    }
   ],
   "source": [
    "APE = PlayTicTacToe.all_possible_endings('-1:1~1:4~-1:9~1:2~-1:5~',False)\n",
    "print(APE)\n",
    "QV = play.train_Q_values(len(APE),PlayTicTacToe.moves_to_dict(APE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-264-9a8793f15767>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mAPE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mGI\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-259-df1871820c3a>\u001b[0m in \u001b[0;36mplay_many\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m             \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprofile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_won\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-259-df1871820c3a>\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[0mmv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minformed_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Random Player\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplyr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m             \u001b[0mprofile\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplyr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"~\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[0mplyr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mother_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplyr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-258-583629025393>\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, action, player)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_won\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__board\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__bad_move_game_is_over\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__invalid_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__bad_move_action_already_played\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__player\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__bad_move_no_consecutive_plays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-258-583629025393>\u001b[0m in \u001b[0;36m__invalid_move\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__invalid_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__board\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-258-583629025393>\u001b[0m in \u001b[0;36mboard_index\u001b[1;34m(cls, action)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mboard_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "GI = {}\n",
    "GR = {}\n",
    "GD = {}\n",
    "APE = {}\n",
    "for i in (0,10):\n",
    "    GI,GR,GD = play.play_many(1000)\n",
    "    print(GR)\n",
    "    if(len(GI)>0):\n",
    "        for key, value in GI.items():\n",
    "            APE = PlayTicTacToe.all_possible_endings(key)\n",
    "            if(len(APE) > 0):\n",
    "                QV = play.train_Q_values(len(APE),PlayTicTacToe.moves_to_dict(APE))\n",
    "    if(len(GR)>0):\n",
    "        for key, value in GR.items():\n",
    "            APE = PlayTicTacToe.all_possible_endings(key)\n",
    "            if(len(APE) > 0):\n",
    "                QV = play.train_Q_values(len(APE),PlayTicTacToe.moves_to_dict(APE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "Informed :548 : 55.0\n",
      "Random :342 : 34.0\n",
      "Draw :110 : 11.0\n",
      "Diff Games :991\n"
     ]
    }
   ],
   "source": [
    "GI = {}\n",
    "GR = {}\n",
    "GD = {}\n",
    "GI,GR,GD = play.play_many(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "play.game().reset()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X : 1000 : Loss : 42.0% Win : 44.0% Draw : 14.0% Play : 0.0% \n",
      "Player O : 1000 : Loss : 44.0% Win : 42.0% Draw : 14.0% Play : 0.0% \n",
      "Player X : 2000 : Loss : 43.0% Win : 44.0% Draw : 13.0% Play : 0.0% \n",
      "Player O : 2000 : Loss : 44.0% Win : 43.0% Draw : 13.0% Play : 0.0% \n",
      "Player X : 3000 : Loss : 43.0% Win : 43.0% Draw : 14.0% Play : 0.0% \n",
      "Player O : 3000 : Loss : 43.0% Win : 43.0% Draw : 14.0% Play : 0.0% \n",
      "Player X : 4000 : Loss : 44.0% Win : 43.0% Draw : 13.0% Play : 0.0% \n",
      "Player O : 4000 : Loss : 43.0% Win : 44.0% Draw : 13.0% Play : 0.0% \n",
      "Player X : 5000 : Loss : 43.0% Win : 44.0% Draw : 13.0% Play : 0.0% \n",
      "Player O : 5000 : Loss : 44.0% Win : 43.0% Draw : 13.0% Play : 0.0% \n",
      "8839\n"
     ]
    }
   ],
   "source": [
    "QV = play.train_Q_values_R(5000)\n",
    "print(len(play.Q_Vals()))\n",
    "QVV = play.Q_Vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_ml(playg):\n",
    "    st = PlayTicTacToe.state(TicTacToe.player_X,playg.game().board())\n",
    "    QV = playg.Q_Vals_for_state(st)\n",
    "    mx = PlayTicTacToe.best_move(playg.Q_Vals_for_state(st))\n",
    "    print(playg.game().board())\n",
    "    print(st)\n",
    "    print(QV)\n",
    "    print(mx)\n",
    "    print(QV==mx)\n",
    "    \n",
    "    playg.game().move(playg.informed_move(st,False),TicTacToe.player_X)\n",
    "    print(playg.game().board())\n",
    "\n",
    "    return\n",
    "\n",
    "def play_me(playg,mv):\n",
    "    st = PlayTicTacToe.state(TicTacToe.player_O,playg.game().board())\n",
    "    QV = playg.Q_Vals_for_state(st)\n",
    "    mx = np.max(playg.Q_Vals_for_state(st))\n",
    "    print(playg.game().board())\n",
    "    playg.game().move(mv,TicTacToe.player_O)\n",
    "    print(playg.game().board())\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "play.game().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1000000000\n",
      "[-0.41894335 -0.418173   -0.41864746 -0.41894011 -0.41869117 -0.41864548\n",
      " -0.41820463 -0.41910538 -0.41832343]\n",
      "-0.418173001435\n",
      "[False  True False False False False False False False]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-4259623ce619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplay_ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-268-7bb5ce22ad71>\u001b[0m in \u001b[0;36mplay_ml\u001b[1;34m(playg)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQV\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mmx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minformed_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Random Player\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mplayg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minformed_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "play_ml(play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0]\n",
      " [ 0  0  0]\n",
      " [ 0  0 -1]]\n",
      "[[-1  1  0]\n",
      " [ 0  0  0]\n",
      " [ 0  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "play_me(play,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sbo(q):\n",
    "    # Best Win\n",
    "    stand_to_lose = np.max((q>=0)*q)\n",
    "    # Least Loss\n",
    "    qnz = q[np.where(q != 0)]\n",
    "    idx = max((-qnz==min(-qnz))*np.nonzero(q)[0])\n",
    "    stand_to_win = q[idx]\n",
    "    if(stand_to_win > stand_to_lose):\n",
    "        return stand_to_win\n",
    "    else:\n",
    "        return stand_to_lose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "q=np.array([-1])\n",
    "print((q>=0)*q)\n",
    "print(-((q<0)*q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def best_move(q):\n",
    "        q = q[np.isnan(q) == False]\n",
    "        if len(q) > 0:\n",
    "            return max(q)\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qv = 0\n",
    "for key, value in play.Q_Vals().items():\n",
    "    if(np.sum((value > 0)*1)>0):\n",
    "        print(key)\n",
    "        print(value)\n",
    "        qv += value\n",
    "print(\"-----\")\n",
    "print(qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0052581\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9])\n",
    "q = np.array([        np.nan ,-0.79800225,         np.nan, -0.80401002, -0.64649276, -0.79928933,\n",
    " -0.0052581,  -0.79975433,         np.nan])\n",
    "bm=best_move(q)\n",
    "print(bm)\n",
    "mvs=(q==bm)*a\n",
    "print(mvs[np.where(mvs!=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[        nan -1.03827974         nan -0.804004           nan -1.03942678\n",
      "         nan -0.804004           nan]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "q = np.array([np.nan, -1.03827974,np.nan, -0.804004,np.nan, -1.03942678,np.nan, -0.804004,np.nan])\n",
    "print(q)\n",
    "print(str(q)==str(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "rn = random.random()\n",
    "rn = np.nan\n",
    "print(str(rn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "l = (1,2,3)\n",
    "print(l[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:\n",
      "nan:\n",
      "-1.0382797399999999:\n",
      "nan:\n",
      "-0.8040040000000001:\n",
      "nan:\n",
      "-1.0394267800000001:\n",
      "nan:\n",
      "-0.8040040000000001:\n",
      "nan:\n"
     ]
    }
   ],
   "source": [
    "print(str(q.size)+\":\")\n",
    "for i in range(0,q.size): print('{:.16f}'.format(q[i])+\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
