{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import math\n",
    "import unittest\n",
    "import operator\n",
    "import numbers\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "\n",
    "    # There are 5812 legal board states that can be reached before there is a winner\n",
    "    # http://brianshourd.com/posts/2012-11-06-tilt-number-of-tic-tac-toe-boards.html\n",
    "    \n",
    "    __bad_move_game_is_over = -1\n",
    "    __bad_move_action_already_played = -2\n",
    "    __bad_move_no_consecutive_plays = -3\n",
    "    __good_move = 0 # value of a free action space on board\n",
    "    __play = 0.0 # reward for playing an action\n",
    "    __draw = 0.0 # reward for playing to end but no one wins\n",
    "    __win = 0.1 # reward for winning a game\n",
    "    __no_player = -2 # id of a non existent player i.e. used to record id of player that has not played\n",
    "    __win_mask = np.full((1, 3),3,np.int8)\n",
    "    actions =  {1:(0,0), 2:(0,1), 3:(0,2), 4:(1,0), 5:(1,1), 6:(1,2), 7:(2,0), 8:(2,1), 9:(2,2)}\n",
    "    __num_actions = 9\n",
    "    player_X = 1\n",
    "    player_O = -1\n",
    "\n",
    "    #\n",
    "    # Return game to intial state, where no one has played\n",
    "    # and the board contains no moves.\n",
    "    #\n",
    "    def reset(self):\n",
    "        self.__board = np.zeros((3, 3),np.int8)\n",
    "        self.__last_board = np.zeros((3, 3),np.int8)\n",
    "        self.__game_over = False\n",
    "        self.__game_drawn = False\n",
    "        self.__player = TicTacToe.__no_player\n",
    "        self.__last_player = TicTacToe.__no_player\n",
    "        #self.__Q = {} - learning spans games !\n",
    "        \n",
    "    #\n",
    "    # Constructor has no arguments as it just sets the game\n",
    "    # to an intial up-played set-up\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__board = np.zeros((3, 3),np.int8)\n",
    "        self.__last_board = np.zeros((3, 3),np.int8)\n",
    "        self.__game_over = False\n",
    "        self.__game_drawn = False\n",
    "        self.__player = TicTacToe.__no_player\n",
    "        self.__last_player = TicTacToe.__no_player\n",
    "        self.__Q = {}\n",
    "    \n",
    "    #\n",
    "    # return player as string \"X\" or \"O\"\n",
    "    #\n",
    "    def __player_to_str(self,player):\n",
    "        if(player == TicTacToe.player_X): return \"X\"\n",
    "        if(player == TicTacToe.player_O): return \"O\"\n",
    "        return \"?\"\n",
    "        \n",
    "    #\n",
    "    # Return a displayable version of the entire game.\n",
    "    #\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"Game Over: \" + str(self.__game_over) +\"\\n\"\n",
    "        s += \"Player :\" + self.__player_to_str(self.__player) + \"\\n\"\n",
    "        s += \"Current Board : \\n\" + str(self.__board)+ \"\\n\"\n",
    "        s += \"Prev Player :\" + self.__player_to_str(self.__last_player) + \"\\n\"\n",
    "        s += \"Prev Current Board : \\n\" + str(self.__last_board)+ \"\\n\"\n",
    "        s += \"State\" + str(self.state()) + \"\\n\"\n",
    "        return s\n",
    "    \n",
    "    #\n",
    "    # Return the list of valid actions\n",
    "    #\n",
    "    def list_actions(self):\n",
    "        return list(TicTacToe.actions)\n",
    "    \n",
    "    #\n",
    "    # Is the game over ?\n",
    "    #\n",
    "    def game_over(self):\n",
    "        return (self.__game_won() or not self.moves_left_to_take())\n",
    "    \n",
    "    #\n",
    "    # Assume the move has been validated by move method\n",
    "    # Make a copy of board before move is made and the last player\n",
    "    #\n",
    "    def __make_move(self, action, player):\n",
    "        self.__last_board = np.copy(self.__board)\n",
    "        self.__last_player = self.__player\n",
    "        self.__player = player\n",
    "        self.__board[TicTacToe.actions[action]] = player\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Has a player already moved using the given action.\n",
    "    #\n",
    "    def __valid_move(self,action):\n",
    "        return self.__board[TicTacToe.actions[action]] != self.__good_move\n",
    "    \n",
    "    #\n",
    "    # If the proposed action is a valid move and the game is not\n",
    "    # over. Make the given move (action) on behalf of the given \n",
    "    # player and update the game status\n",
    "    #\n",
    "    def move(self, action, player):\n",
    "        if(self.__game_won()) : return TicTacToe.__bad_move_game_is_over\n",
    "        if(self.__valid_move(action)): return TicTacToe.__bad_move_action_already_played \n",
    "        if(player == self.__player): return TicTacToe.__bad_move_no_consecutive_plays \n",
    "        \n",
    "        self.__make_move(action,player)\n",
    "\n",
    "        if(self.__game_won()):\n",
    "            self.__game_over = True\n",
    "            self.__game_drawn = False\n",
    "            return TicTacToe.__win\n",
    "            \n",
    "        if(not self.moves_left_to_take()):\n",
    "            self.__game_over = True\n",
    "            self.__game_drawn = True\n",
    "            return TicTacToe.__draw\n",
    "\n",
    "        return TicTacToe.__play            \n",
    "\n",
    "    #\n",
    "    # Return (flattened) Game Ended, Last Player, Last Board, Player, Board\n",
    "    #\n",
    "    def detailed_state(self):\n",
    "        flattened_state = []\n",
    "        if(self.__game_over):\n",
    "            flattened_state.append(1)\n",
    "        else:\n",
    "            flattened_state.append(0)\n",
    "        flattened_state.append(self.__last_player)\n",
    "        flattened_state.append(self.__player)\n",
    "        for itm in np.reshape(self.__last_board,9).tolist() : flattened_state.append(itm)\n",
    "        for itm in np.reshape(self.__board,9).tolist() : flattened_state.append(itm)\n",
    "            \n",
    "        return flattened_state\n",
    "\n",
    "    #\n",
    "    # Return state of current board as simple vector\n",
    "    #\n",
    "    def state(self,tostr=False):\n",
    "        flattened_state = []\n",
    "        flattened_state.append(self.__player)\n",
    "        for itm in np.reshape(self.__board,9).tolist() : flattened_state.append(itm)            \n",
    "        if not tostr:\n",
    "            return flattened_state\n",
    "        else:\n",
    "            return ''.join(str(e) for e in flattened_state)\n",
    "\n",
    "    #\n",
    "    # Show return the current board contents\n",
    "    #\n",
    "    def board(self):\n",
    "        return self.__board\n",
    "     \n",
    "    #\n",
    "    # Any row, column or diagonal with all player X or player O\n",
    "    #\n",
    "    def __game_won(self):\n",
    "        rows = np.abs(np.sum(self.__board,axis=1))\n",
    "        cols = np.abs(np.sum(self.__board,axis=0))\n",
    "        diagLR = np.abs(np.sum(self.__board.diagonal()))\n",
    "        diagRL = np.abs(np.sum(np.rot90(self.__board).diagonal()))\n",
    "        \n",
    "        if(np.sum(rows == self.__win_mask) > 0):\n",
    "            return True\n",
    "        if(np.sum(cols == self.__win_mask) > 0):\n",
    "            return True\n",
    "        if((np.mod(diagLR,3)) == 0) and diagLR > 0:\n",
    "            return True\n",
    "        if((np.mod(diagRL,3)) == 0) and diagRL > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #\n",
    "    # Forget learning\n",
    "    #\n",
    "    def forget(self):\n",
    "        self.__Q = {}\n",
    "\n",
    "    #\n",
    "    # Return which player goes next given the current player\n",
    "    #\n",
    "    def next_player(self,current_player):\n",
    "        if(current_player == TicTacToe.player_O):\n",
    "            return  TicTacToe.player_X\n",
    "        else:\n",
    "            return  TicTacToe.player_O\n",
    "\n",
    "    #\n",
    "    # Are there any remaining moves to be taken >\n",
    "    #\n",
    "    def moves_left_to_take(self):\n",
    "        return (self.__board[np.where(self.__board == 0)]).size > 0\n",
    "    \n",
    "    #\n",
    "    # Return a random action (move) that is still left\n",
    "    # to make\n",
    "    #\n",
    "    def random_move(self):\n",
    "        valid_moves = []\n",
    "        random_action = None\n",
    "        for key in self.list_actions():\n",
    "            if(self.__board[TicTacToe.actions[key]] == self.__good_move):\n",
    "                valid_moves.append(key)\n",
    "         \n",
    "        num_poss_moves = len(valid_moves)\n",
    "        if(num_poss_moves > 0):\n",
    "            random_action = valid_moves[randint(0, num_poss_moves-1)]\n",
    "            return random_action\n",
    "        else:\n",
    "            return None\n",
    "    #\n",
    "    # Return a informed action (move) that is still left\n",
    "    # to make based on given Q values for actions from a \n",
    "    # given state\n",
    "    #\n",
    "    def informed_move(self):\n",
    "        # What moves are possible at this stage\n",
    "        valid_moves = np.zeros(self.__num_actions)\n",
    "        best_action = None\n",
    "        for key in self.list_actions():\n",
    "            if(self.__board[TicTacToe.actions[key]] == self.__good_move):\n",
    "                valid_moves[key-1] = True\n",
    "            else:\n",
    "                valid_moves[key-1] = False\n",
    "        \n",
    "        # Are there any moves ? \n",
    "        if(np.sum(valid_moves*np.full(self.__num_actions,1)) == 0):\n",
    "            return None\n",
    "           \n",
    "        # Is there info learned for this state ?\n",
    "        if self.state(tostr=True) in self.__Q:\n",
    "            informed_actions = self.__Q[self.state(tostr=True)]\n",
    "            informed_actions *= valid_moves\n",
    "            best_action = np.max(informed_actions)\n",
    "            if(best_action > 0):\n",
    "                informed_actions = np.arange(1,self.__num_actions+1,1)[np.where(informed_actions == best_action)]\n",
    "                best_action = informed_actions[randint(0, informed_actions.size-1)]\n",
    "            else:\n",
    "                best_action = None\n",
    "\n",
    "        # If we found a good action then return that \n",
    "        # else pick a random action\n",
    "        if best_action == None:\n",
    "            actions = valid_moves*np.arange(1,self.__num_actions+1,1)\n",
    "            actions = actions[np.where(actions > 0)]\n",
    "            best_action = actions[randint(0,actions.size-1)]\n",
    "\n",
    "        return int(best_action)\n",
    "\n",
    "    #\n",
    "    # Play a random game until completion.\n",
    "    #\n",
    "    def play(self):\n",
    "        self.reset()\n",
    "        plyr = (TicTacToe.player_X,TicTacToe.player_O)[randint(0,1)] # Random player to start\n",
    "        mv = None\n",
    "        while(not self.game_over()):\n",
    "            print(self.board())\n",
    "            print(self.__Q[self.state(tostr=True)])\n",
    "            mv = self.informed_move()\n",
    "            self.move(mv,plyr)\n",
    "            plyr = self.next_player(plyr)\n",
    "        \n",
    "        print(\"-\")\n",
    "        print(self.board())\n",
    "        print(self.__Q[self.state(tostr=True)])\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Run simulation to estimate Q values for state, action pairs. Random exploration policy\n",
    "    # which should be tractable with approx 6K valid board states.\n",
    "    #\n",
    "    def estimate_Q_values(self,num_simulations):\n",
    "        exploration = 1.0\n",
    "        decay = (1.0/num_simulations)\n",
    "        learning_rate0 = 0.05\n",
    "        learning_rate_decay = 0.1\n",
    "        discount_rate = 0.95\n",
    "        reward = 0\n",
    "        s = None\n",
    "        sp = None\n",
    "        sim = 0 \n",
    "        score = {TicTacToe.__draw:0,TicTacToe.__win:0}\n",
    "        while(sim < num_simulations):\n",
    "            self.reset()\n",
    "            plyr = (TicTacToe.player_X,TicTacToe.player_O)[randint(0,1)] # Random player to start\n",
    "            mv = None\n",
    "            while(not self.game_over()):\n",
    "                s = self.state(tostr=True)\n",
    "                if(True):#random.random() < (exploration-(decay*sim))):\n",
    "                    mv = self.random_move()\n",
    "                else:\n",
    "                    mv = self.informed_move()\n",
    "                reward = self.move(mv,plyr)\n",
    "                sp = self.state(tostr=True)\n",
    "                learning_rate = learning_rate0 / (1 + (sim * learning_rate_decay))\n",
    "                if s not in self.__Q:\n",
    "                    self.__Q[s] = np.zeros((self.__num_actions))\n",
    "                if sp not in self.__Q:\n",
    "                    self.__Q[sp] = np.zeros((self.__num_actions))\n",
    "                (self.__Q[s])[mv-1] = learning_rate * (self.__Q[s])[mv-1] + (1-learning_rate) * (reward + discount_rate * np.max(self.__Q[sp]))\n",
    "                plyr = self.next_player(plyr)\n",
    "            sim += 1\n",
    "            score[reward] += 1\n",
    "            if (sim % 1000) == 0 : \n",
    "                print(str(sim)+\" Win : \"+str(round((score[TicTacToe.__win]/sim)*100,0))+\"% Draw: \" + str(round((score[TicTacToe.__draw]/sim)*100,0))+\"%\")\n",
    "        print(score)\n",
    "        return self.__Q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Win : 92.0% Draw: 8.0%\n",
      "200 Win : 86.0% Draw: 14.0%\n",
      "300 Win : 84.0% Draw: 16.0%\n",
      "400 Win : 87.0% Draw: 13.0%\n",
      "500 Win : 86.0% Draw: 14.0%\n",
      "600 Win : 86.0% Draw: 14.0%\n",
      "700 Win : 86.0% Draw: 14.0%\n",
      "800 Win : 86.0% Draw: 14.0%\n",
      "900 Win : 86.0% Draw: 14.0%\n",
      "1000 Win : 86.0% Draw: 14.0%\n",
      "1100 Win : 87.0% Draw: 13.0%\n",
      "1200 Win : 87.0% Draw: 13.0%\n",
      "1300 Win : 88.0% Draw: 12.0%\n",
      "1400 Win : 88.0% Draw: 12.0%\n",
      "1500 Win : 88.0% Draw: 12.0%\n",
      "1600 Win : 88.0% Draw: 12.0%\n",
      "1700 Win : 88.0% Draw: 12.0%\n",
      "1800 Win : 88.0% Draw: 12.0%\n",
      "1900 Win : 88.0% Draw: 12.0%\n",
      "2000 Win : 88.0% Draw: 12.0%\n",
      "2100 Win : 88.0% Draw: 12.0%\n",
      "2200 Win : 88.0% Draw: 12.0%\n",
      "2300 Win : 88.0% Draw: 12.0%\n",
      "2400 Win : 88.0% Draw: 12.0%\n",
      "2500 Win : 88.0% Draw: 12.0%\n",
      "2600 Win : 88.0% Draw: 12.0%\n",
      "2700 Win : 88.0% Draw: 12.0%\n",
      "2800 Win : 88.0% Draw: 12.0%\n",
      "2900 Win : 88.0% Draw: 12.0%\n",
      "3000 Win : 88.0% Draw: 12.0%\n",
      "3100 Win : 88.0% Draw: 12.0%\n",
      "3200 Win : 88.0% Draw: 12.0%\n",
      "3300 Win : 88.0% Draw: 12.0%\n",
      "3400 Win : 88.0% Draw: 12.0%\n",
      "3500 Win : 88.0% Draw: 12.0%\n",
      "3600 Win : 88.0% Draw: 12.0%\n",
      "3700 Win : 88.0% Draw: 12.0%\n",
      "3800 Win : 88.0% Draw: 12.0%\n",
      "3900 Win : 88.0% Draw: 12.0%\n",
      "4000 Win : 88.0% Draw: 12.0%\n",
      "4100 Win : 88.0% Draw: 12.0%\n",
      "4200 Win : 88.0% Draw: 12.0%\n",
      "4300 Win : 88.0% Draw: 12.0%\n",
      "4400 Win : 88.0% Draw: 12.0%\n",
      "4500 Win : 88.0% Draw: 12.0%\n",
      "4600 Win : 88.0% Draw: 12.0%\n",
      "4700 Win : 88.0% Draw: 12.0%\n",
      "4800 Win : 88.0% Draw: 12.0%\n",
      "4900 Win : 87.0% Draw: 13.0%\n",
      "5000 Win : 88.0% Draw: 12.0%\n",
      "5100 Win : 87.0% Draw: 13.0%\n",
      "5200 Win : 87.0% Draw: 13.0%\n",
      "5300 Win : 87.0% Draw: 13.0%\n",
      "5400 Win : 87.0% Draw: 13.0%\n",
      "5500 Win : 87.0% Draw: 13.0%\n",
      "5600 Win : 87.0% Draw: 13.0%\n",
      "5700 Win : 87.0% Draw: 13.0%\n",
      "5800 Win : 87.0% Draw: 13.0%\n",
      "5900 Win : 87.0% Draw: 13.0%\n",
      "6000 Win : 87.0% Draw: 13.0%\n",
      "6100 Win : 87.0% Draw: 13.0%\n",
      "6200 Win : 87.0% Draw: 13.0%\n",
      "6300 Win : 87.0% Draw: 13.0%\n",
      "6400 Win : 87.0% Draw: 13.0%\n",
      "6500 Win : 87.0% Draw: 13.0%\n",
      "6600 Win : 87.0% Draw: 13.0%\n",
      "6700 Win : 87.0% Draw: 13.0%\n",
      "6800 Win : 87.0% Draw: 13.0%\n",
      "6900 Win : 87.0% Draw: 13.0%\n",
      "7000 Win : 87.0% Draw: 13.0%\n",
      "7100 Win : 87.0% Draw: 13.0%\n",
      "7200 Win : 87.0% Draw: 13.0%\n",
      "7300 Win : 87.0% Draw: 13.0%\n",
      "7400 Win : 87.0% Draw: 13.0%\n",
      "7500 Win : 87.0% Draw: 13.0%\n",
      "7600 Win : 87.0% Draw: 13.0%\n",
      "7700 Win : 87.0% Draw: 13.0%\n",
      "7800 Win : 87.0% Draw: 13.0%\n",
      "7900 Win : 87.0% Draw: 13.0%\n",
      "8000 Win : 87.0% Draw: 13.0%\n",
      "8100 Win : 87.0% Draw: 13.0%\n",
      "8200 Win : 87.0% Draw: 13.0%\n",
      "8300 Win : 87.0% Draw: 13.0%\n",
      "8400 Win : 87.0% Draw: 13.0%\n",
      "8500 Win : 87.0% Draw: 13.0%\n",
      "8600 Win : 87.0% Draw: 13.0%\n",
      "8700 Win : 87.0% Draw: 13.0%\n",
      "8800 Win : 87.0% Draw: 13.0%\n",
      "8900 Win : 87.0% Draw: 13.0%\n",
      "9000 Win : 87.0% Draw: 13.0%\n",
      "9100 Win : 87.0% Draw: 13.0%\n",
      "9200 Win : 87.0% Draw: 13.0%\n",
      "9300 Win : 87.0% Draw: 13.0%\n",
      "9400 Win : 87.0% Draw: 13.0%\n",
      "9500 Win : 87.0% Draw: 13.0%\n",
      "9600 Win : 87.0% Draw: 13.0%\n",
      "9700 Win : 87.0% Draw: 13.0%\n",
      "9800 Win : 87.0% Draw: 13.0%\n",
      "9900 Win : 87.0% Draw: 13.0%\n",
      "10000 Win : 87.0% Draw: 13.0%\n",
      "10100 Win : 87.0% Draw: 13.0%\n",
      "10200 Win : 87.0% Draw: 13.0%\n",
      "10300 Win : 87.0% Draw: 13.0%\n",
      "10400 Win : 87.0% Draw: 13.0%\n",
      "10500 Win : 87.0% Draw: 13.0%\n",
      "10600 Win : 87.0% Draw: 13.0%\n",
      "10700 Win : 87.0% Draw: 13.0%\n",
      "10800 Win : 87.0% Draw: 13.0%\n",
      "10900 Win : 87.0% Draw: 13.0%\n",
      "11000 Win : 87.0% Draw: 13.0%\n",
      "11100 Win : 87.0% Draw: 13.0%\n",
      "11200 Win : 87.0% Draw: 13.0%\n",
      "11300 Win : 87.0% Draw: 13.0%\n",
      "11400 Win : 87.0% Draw: 13.0%\n",
      "11500 Win : 87.0% Draw: 13.0%\n",
      "11600 Win : 87.0% Draw: 13.0%\n",
      "11700 Win : 87.0% Draw: 13.0%\n",
      "11800 Win : 87.0% Draw: 13.0%\n",
      "11900 Win : 87.0% Draw: 13.0%\n",
      "12000 Win : 87.0% Draw: 13.0%\n",
      "12100 Win : 87.0% Draw: 13.0%\n",
      "12200 Win : 87.0% Draw: 13.0%\n",
      "12300 Win : 87.0% Draw: 13.0%\n",
      "12400 Win : 87.0% Draw: 13.0%\n",
      "12500 Win : 87.0% Draw: 13.0%\n",
      "12600 Win : 87.0% Draw: 13.0%\n",
      "12700 Win : 87.0% Draw: 13.0%\n",
      "12800 Win : 87.0% Draw: 13.0%\n",
      "12900 Win : 87.0% Draw: 13.0%\n",
      "13000 Win : 87.0% Draw: 13.0%\n",
      "13100 Win : 87.0% Draw: 13.0%\n",
      "13200 Win : 87.0% Draw: 13.0%\n",
      "13300 Win : 87.0% Draw: 13.0%\n",
      "13400 Win : 87.0% Draw: 13.0%\n",
      "13500 Win : 87.0% Draw: 13.0%\n",
      "13600 Win : 87.0% Draw: 13.0%\n",
      "13700 Win : 87.0% Draw: 13.0%\n",
      "13800 Win : 87.0% Draw: 13.0%\n",
      "13900 Win : 87.0% Draw: 13.0%\n",
      "14000 Win : 87.0% Draw: 13.0%\n",
      "14100 Win : 87.0% Draw: 13.0%\n",
      "14200 Win : 87.0% Draw: 13.0%\n",
      "14300 Win : 87.0% Draw: 13.0%\n",
      "14400 Win : 87.0% Draw: 13.0%\n",
      "14500 Win : 87.0% Draw: 13.0%\n",
      "14600 Win : 87.0% Draw: 13.0%\n",
      "14700 Win : 87.0% Draw: 13.0%\n",
      "14800 Win : 87.0% Draw: 13.0%\n",
      "14900 Win : 87.0% Draw: 13.0%\n",
      "15000 Win : 87.0% Draw: 13.0%\n",
      "15100 Win : 87.0% Draw: 13.0%\n",
      "15200 Win : 87.0% Draw: 13.0%\n",
      "15300 Win : 87.0% Draw: 13.0%\n",
      "15400 Win : 87.0% Draw: 13.0%\n",
      "15500 Win : 87.0% Draw: 13.0%\n",
      "15600 Win : 87.0% Draw: 13.0%\n",
      "15700 Win : 87.0% Draw: 13.0%\n",
      "15800 Win : 87.0% Draw: 13.0%\n",
      "15900 Win : 87.0% Draw: 13.0%\n",
      "16000 Win : 87.0% Draw: 13.0%\n",
      "16100 Win : 87.0% Draw: 13.0%\n",
      "16200 Win : 87.0% Draw: 13.0%\n",
      "16300 Win : 87.0% Draw: 13.0%\n",
      "16400 Win : 87.0% Draw: 13.0%\n",
      "16500 Win : 87.0% Draw: 13.0%\n",
      "16600 Win : 87.0% Draw: 13.0%\n",
      "16700 Win : 87.0% Draw: 13.0%\n",
      "16800 Win : 87.0% Draw: 13.0%\n",
      "16900 Win : 87.0% Draw: 13.0%\n",
      "17000 Win : 87.0% Draw: 13.0%\n",
      "17100 Win : 87.0% Draw: 13.0%\n",
      "17200 Win : 87.0% Draw: 13.0%\n",
      "17300 Win : 87.0% Draw: 13.0%\n",
      "17400 Win : 87.0% Draw: 13.0%\n",
      "17500 Win : 87.0% Draw: 13.0%\n",
      "17600 Win : 87.0% Draw: 13.0%\n",
      "17700 Win : 87.0% Draw: 13.0%\n",
      "17800 Win : 87.0% Draw: 13.0%\n",
      "17900 Win : 87.0% Draw: 13.0%\n",
      "18000 Win : 87.0% Draw: 13.0%\n",
      "18100 Win : 87.0% Draw: 13.0%\n",
      "18200 Win : 87.0% Draw: 13.0%\n",
      "18300 Win : 87.0% Draw: 13.0%\n",
      "18400 Win : 87.0% Draw: 13.0%\n",
      "18500 Win : 87.0% Draw: 13.0%\n",
      "18600 Win : 87.0% Draw: 13.0%\n",
      "18700 Win : 87.0% Draw: 13.0%\n",
      "18800 Win : 87.0% Draw: 13.0%\n",
      "18900 Win : 87.0% Draw: 13.0%\n",
      "19000 Win : 87.0% Draw: 13.0%\n",
      "19100 Win : 87.0% Draw: 13.0%\n",
      "19200 Win : 87.0% Draw: 13.0%\n",
      "19300 Win : 87.0% Draw: 13.0%\n",
      "19400 Win : 87.0% Draw: 13.0%\n",
      "19500 Win : 87.0% Draw: 13.0%\n",
      "19600 Win : 87.0% Draw: 13.0%\n",
      "19700 Win : 87.0% Draw: 13.0%\n",
      "19800 Win : 87.0% Draw: 13.0%\n",
      "19900 Win : 87.0% Draw: 13.0%\n",
      "20000 Win : 87.0% Draw: 13.0%\n",
      "{0.0: 2612, 0.01: 17388}\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "game = TicTacToe()\n",
    "QV = game.estimate_Q_values(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[ 0.00814506  0.00814506  0.00814506  0.00814506  0.00814506  0.00814506\n",
      "  0.00814506  0.00814506  0.00814506]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "[ 0.00857375  0.00857375  0.00857375  0.00857375  0.00857375  0.00857375\n",
      "  0.00857375  0.          0.00857375]\n",
      "[[ 0  0  0]\n",
      " [ 0  0  0]\n",
      " [-1  1  0]]\n",
      "[ 0.00857375  0.009025    0.00857375  0.00857141  0.009025    0.00857375\n",
      "  0.          0.          0.00857375]\n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  1  0]]\n",
      "[ 0.0095      0.00857375  0.00948333  0.0095      0.          0.00949967\n",
      "  0.          0.          0.0095    ]\n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  1 -1]]\n",
      "[ 0.00902389  0.01        0.00902474  0.00857333  0.          0.0085673   0.\n",
      "  0.          0.        ]\n",
      "-\n",
      "[[ 0  1  0]\n",
      " [ 0  1  0]\n",
      " [-1  1 -1]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()\n",
    "n = 0\n",
    "go = True\n",
    "while(go and n < 100):\n",
    "    go = game.play()\n",
    "    sys.stdout.write(\".\")\n",
    "    n+=1\n",
    "print(\"\\n\")\n",
    "print(game.board())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1 -1]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "bd = game.board()\n",
    "print(bd)\n",
    "#return (self.__board[np.where(self.__board == 0)]).size > 0\n",
    "print((bd[np.where(bd ==0)].size)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class testTicTacToe(unittest.TestCase):\n",
    "    def test_one(self):\n",
    "        self.assertEqual(1 == 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Run A Single Test\n",
    "#\n",
    "test_to_run = \"test_one\"\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(testTicTacToe(test_to_run))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Run All Tests.\n",
    "#\n",
    "tests = testTicTacToe()\n",
    "suite = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = np.array([True,False,True,False,False,True,False,False,True])\n",
    "qv = np.array([1,3,1,0,2])\n",
    "ac = np.array([1,2,3,4,5])\n",
    "im = (vm*qv)[np.where((vm*qv) == np.max(vm*qv))]\n",
    "print(im)\n",
    "mv = (vm*ac)[np.where(vm*ac > 0)]\n",
    "print(mv.size)\n",
    "print(mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_moves = np.array([True,False,True,False,False,True,False,False,True])\n",
    "print(valid_moves)\n",
    "actions = valid_moves*np.arange(1,9+1,1)\n",
    "print(actions)\n",
    "actions = actions[np.where(actions > 0)]\n",
    "print(actions)\n",
    "print(actions.size)\n",
    "best_action = actions[randint(0, actions.size-1)]\n",
    "print(best_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class playTicTacToe:\n",
    "    __actor_network_name = \"net/actor\"\n",
    "    __critic_network_name = \"net/critic\"\n",
    "    __x_units = 21\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Build a four layer fully connected Network.\n",
    "    # 21 -> relu -> 21 -> relu -> 21 -> relu -> 21 -> relu\n",
    "    #\n",
    "    def construct_network(X_state,network_name):\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
