{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import math\n",
    "import unittest\n",
    "import operator\n",
    "import numbers\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "\n",
    "    # There are 5812 legal board states that can be reached before there is a winner\n",
    "    # http://brianshourd.com/posts/2012-11-06-tilt-number-of-tic-tac-toe-boards.html\n",
    "    \n",
    "    __bad_move_game_is_over = -1\n",
    "    __bad_move_action_already_played = -2\n",
    "    __bad_move_no_consecutive_plays = -3\n",
    "    __play = float(0) # reward for playing an action\n",
    "    __draw = float(10) # reward for playing to end but no one wins\n",
    "    __win = float(5) # reward for winning a game\n",
    "    __loss = float(-10) # reward (penalty) for losing a game\n",
    "    __rewards = {\"Play\":TicTacToe.__play,\"Draw\":TicTacToe.__draw,\"Win\":TicTacToe.__win,\"Loss\":TicTacToe.__loss}\n",
    "    __no_player = -2 # id of a non existent player i.e. used to record id of player that has not played\n",
    "    __win_mask = np.full((1, 3),3,np.int8)\n",
    "    __actions = {1:(0,0), 2:(0,1), 3:(0,2), 4:(1,0), 5:(1,1), 6:(1,2), 7:(2,0), 8:(2,1), 9:(2,2)}\n",
    "    player_X = 1 # numerical value of player X on the board\n",
    "    player_O = -1 # numerical value of player O on the board\n",
    "    empty_cell = 0 # value of a free action space on board\n",
    "    asStr = True\n",
    "\n",
    "    #\n",
    "    # Return game to intial state, where no one has played\n",
    "    # and the board contains no moves.\n",
    "    #\n",
    "    def reset(self):\n",
    "        self.__board = np.zeros((3, 3),np.int8)\n",
    "        self.__last_board = np.zeros((3, 3),np.int8)\n",
    "        self.__game_over = False\n",
    "        self.__game_drawn = False\n",
    "        self.__player = TicTacToe.__no_player\n",
    "        self.__last_player = TicTacToe.__no_player\n",
    "        \n",
    "    #\n",
    "    # Constructor has no arguments as it just sets the game\n",
    "    # to an intial up-played set-up\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__board = np.zeros((3, 3),np.int8)\n",
    "        self.__last_board = np.zeros((3, 3),np.int8)\n",
    "        self.__game_over = False\n",
    "        self.__game_drawn = False\n",
    "        self.__player = TicTacToe.__no_player\n",
    "        self.__last_player = TicTacToe.__no_player\n",
    "    \n",
    "    #\n",
    "    # Return a displayable version of the entire game.\n",
    "    #\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        s += \"Game Over: \" + str(self.__game_over) +\"\\n\"\n",
    "        s += \"Player :\" + TicTacToe.__player_to_str(self.__player) + \"\\n\"\n",
    "        s += \"Current Board : \\n\" + str(self.__board)+ \"\\n\"\n",
    "        s += \"Prev Player :\" + TicTacToe.__player_to_str(self.__last_player) + \"\\n\"\n",
    "        s += \"Prev Current Board : \\n\" + str(self.__last_board)+ \"\\n\"\n",
    "        s += \"State\" + str(self.state()) + \"\\n\"\n",
    "        return s\n",
    "    \n",
    "    #\n",
    "    # return player as string \"X\" or \"O\"\n",
    "    #\n",
    "    @classmethod\n",
    "    def __player_to_str(cls,self,player):\n",
    "        if(player == TicTacToe.player_X): return \"X\"\n",
    "        if(player == TicTacToe.player_O): return \"O\"\n",
    "        return \"?\"\n",
    "        \n",
    "    #\n",
    "    # Return the actions as a list of integers.\n",
    "    #\n",
    "    @classmethod\n",
    "    def num_actions(cls):\n",
    "        return len(TicTacToe.__actions)\n",
    "\n",
    "    #\n",
    "    # Return the actions as a list of integers.\n",
    "    #\n",
    "    @classmethod\n",
    "    def actions(cls):\n",
    "        return list(map(lambda a: int(a), list(TicTacToe.__actions.keys())))\n",
    "\n",
    "    #\n",
    "    # Return the board index (i,j) of a given action\n",
    "    #\n",
    "    @classmethod\n",
    "    def board_index(cls,action):\n",
    "        return TicTacToe.__actions[action]\n",
    "\n",
    "    #\n",
    "    # Return rewards as dictionary where key is name of reward\n",
    "    # and the value is the reward\n",
    "    #\n",
    "    @classmethod\n",
    "    def rewards(cls):\n",
    "        return TicTacToe.__rewards\n",
    "    \n",
    "    #\n",
    "    # Assume the move has been validated by move method\n",
    "    # Make a copy of board before move is made and the last player\n",
    "    #\n",
    "    def __make_move(self, action, player):\n",
    "        self.__last_board = np.copy(self.__board)\n",
    "        self.__last_player = self.__player\n",
    "        self.__player = player\n",
    "        self.__board[TicTacToe.board_index(action)] = player\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Has a player already moved using the given action.\n",
    "    #\n",
    "    def __valid_move(self,action):\n",
    "        return self.__board[TicTacToe.board_index(action)] != TicTacToe.empty_cell\n",
    "    \n",
    "    #\n",
    "    # If the proposed action is a valid move and the game is not\n",
    "    # over. Make the given move (action) on behalf of the given \n",
    "    # player and update the game status.\n",
    "    #\n",
    "    # return the rawards (Player who took move, Observer)\n",
    "    #\n",
    "    def move(self, action, player):\n",
    "        if(TicTacToe.game_won(self.__board)) : return TicTacToe.__bad_move_game_is_over\n",
    "        if(self.__valid_move(action)): return TicTacToe.__bad_move_action_already_played \n",
    "        if(player == self.__player): return TicTacToe.__bad_move_no_consecutive_plays \n",
    "        \n",
    "        self.__make_move(action,player)\n",
    "\n",
    "        if(TicTacToe.game_won(self.__board)):\n",
    "            self.__game_over = True\n",
    "            self.__game_drawn = False\n",
    "            return np.array([TicTacToe.__win,TicTacToe.__loss])\n",
    "            \n",
    "        if(not TicTacToe.moves_left_to_take(self.__board)):\n",
    "            self.__game_over = True\n",
    "            self.__game_drawn = True\n",
    "            return np.array([TicTacToe.__draw,TicTacToe.__draw])\n",
    "\n",
    "        return np.array([TicTacToe.__play,0])\n",
    "\n",
    "    #\n",
    "    # Return (flattened) Game Ended, Last Player, Last Board, Player, Board\n",
    "    #\n",
    "    def detailed_state(self):\n",
    "        flattened_state = []\n",
    "        if(self.__game_over):\n",
    "            flattened_state.append(1)\n",
    "        else:\n",
    "            flattened_state.append(0)\n",
    "        flattened_state.append(self.__last_player)\n",
    "        flattened_state.append(self.__player)\n",
    "        for itm in np.reshape(self.__last_board,9).tolist() : flattened_state.append(itm)\n",
    "        for itm in np.reshape(self.__board,9).tolist() : flattened_state.append(itm)\n",
    "            \n",
    "        return flattened_state\n",
    "\n",
    "    #\n",
    "    # Show return the current board contents\n",
    "    #\n",
    "    def board(self):\n",
    "        return self.__board\n",
    "     \n",
    "    #\n",
    "    # Any row, column or diagonal with all player X or player O. If a\n",
    "    # player is given then it answers has that specific player won\n",
    "    #\n",
    "    @classmethod\n",
    "    def game_won(cls,bd,plyr=None):\n",
    "    \n",
    "        if not plyr is None: bd = (bd==plyr)*1\n",
    "        \n",
    "        rows = np.abs(np.sum(bd,axis=1))\n",
    "        cols = np.abs(np.sum(bd,axis=0))\n",
    "        diagLR = np.abs(np.sum(bd.diagonal()))\n",
    "        diagRL = np.abs(np.sum(np.rot90(bd).diagonal()))        \n",
    "    \n",
    "        if(np.sum(rows == 3) > 0):\n",
    "            return True\n",
    "        if(np.sum(cols == 3) > 0):\n",
    "            return True\n",
    "        if((np.mod(diagLR,3)) == 0) and diagLR > 0:\n",
    "            return True\n",
    "        if((np.mod(diagRL,3)) == 0) and diagRL > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #\n",
    "    # Are there any remaining moves to be taken >\n",
    "    #\n",
    "    @classmethod\n",
    "    def moves_left_to_take(cls,board):\n",
    "        return (board[np.where(board == 0)]).size > 0\n",
    "    \n",
    "    #\n",
    "    # Board is in a gamne over state, with a winner or a draw\n",
    "    #\n",
    "    @classmethod\n",
    "    def board_game_over(cls,board):\n",
    "        return (TicTacToe.game_won(board) or not TicTacToe.moves_left_to_take(board))\n",
    "        \n",
    "    #\n",
    "    # Is the game over ?\n",
    "    #\n",
    "    def game_over(self):\n",
    "        return TicTacToe.board_game_over(self.__board)\n",
    "    \n",
    "    #\n",
    "    # Return which player goes next given the current player\n",
    "    #\n",
    "    @staticmethod\n",
    "    def other_player(current_player):\n",
    "        if(current_player == TicTacToe.player_O):\n",
    "            return  TicTacToe.player_X\n",
    "        else:\n",
    "            return  TicTacToe.player_O\n",
    "\n",
    "    #\n",
    "    # What moves are valid for the given board\n",
    "    #\n",
    "    @classmethod\n",
    "    def valid_moves(cls,board):\n",
    "        vm = np.zeros(TicTacToe.num_actions())\n",
    "        best_action = None\n",
    "        for actn in TicTacToe.actions():\n",
    "            if(board[TicTacToe.board_index(actn)] == 0):\n",
    "                vm[int(actn)-1] = True\n",
    "            else:\n",
    "                vm[int(actn)-1] = False\n",
    "        return vm\n",
    "    \n",
    "    #\n",
    "    # What moves are valid given for board or if not\n",
    "    # for the current game board.\n",
    "    #\n",
    "    def what_are_valid_moves(self):\n",
    "        return TicTacToe.valid_moves(self.__board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayTicTacToe:\n",
    "\n",
    "    #\n",
    "    # Constructor has no arguments as it just sets the game\n",
    "    # to an intial up-played set-up\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__game = TicTacToe()\n",
    "        self.__Q = {}\n",
    "    \n",
    "    #\n",
    "    # Return the current game\n",
    "    #\n",
    "    def game(self):\n",
    "        return self.__game\n",
    "    \n",
    "    #\n",
    "    # Set leared state to given QValues.\n",
    "    #\n",
    "    def transfer_learning(self,QV):\n",
    "        self.__Q = QV\n",
    "        print(\"Learned Games:\" + str(len(self.__Q)))\n",
    "        \n",
    "    #\n",
    "    # The learned Q Values for a given state if they exist\n",
    "    #\n",
    "    def Q_Vals_for_state(self,state):\n",
    "        if(state in self.__Q):\n",
    "            return(self.__Q[state])\n",
    "        else:\n",
    "            return(None)\n",
    "\n",
    "    #\n",
    "    # Expose the current class instance learning in terms of Q Values.\n",
    "    #\n",
    "    def Q_Vals(self):\n",
    "        return(self.__Q)\n",
    "    \n",
    "    #\n",
    "    # Forget learning\n",
    "    #\n",
    "    def forget_learning(self):\n",
    "        self.__Q = {}\n",
    "\n",
    "    #\n",
    "    # Add states to Q Value dictionary if not present\n",
    "    #\n",
    "    def add_states_if_missing(self,s1,sp1):\n",
    "        if s1 not in self.__Q:\n",
    "            self.__Q[s1] = np.zeros(TicTacToe.num_actions())\n",
    "        if sp1 not in self.__Q:\n",
    "            self.__Q[sp1] = np.zeros(TicTacToe.num_actions())\n",
    "\n",
    "    #\n",
    "    # Update the Q values for the given player state and\n",
    "    # the given reward\n",
    "    #\n",
    "    def update_Q_Values_for_player(self,mv,s,sp,reward,learning_rate,discount_rate):\n",
    "        \n",
    "        actn = mv-1 # action is indexed from zero, moves are 1..9\n",
    "        (self.__Q[s])[actn] = learning_rate * (self.__Q[s])[actn] + (1-learning_rate) * (reward + discount_rate * -np.max(self.__Q[sp]))\n",
    "            \n",
    "        print(\"[\"+ s +\"] - > [\" + sp +\"]\")\n",
    "        print(\"[\"+ str(self.__Q[s]) +\"] - > [\" + str(self.__Q[sp]) +\"]\")\n",
    "        print(\">>\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Keep Score of players as Q Val Trains.\n",
    "    #\n",
    "    @classmethod\n",
    "    def __init_score(cls):\n",
    "        score = {}\n",
    "        score[TicTacToe.player_X]={}\n",
    "        score[TicTacToe.player_O]={}\n",
    "        for rn,rv in TicTacToe.rewards().items():\n",
    "            score[TicTacToe.player_X][rv] = 0\n",
    "            score[TicTacToe.player_O][rv] = 0\n",
    "        return score\n",
    "\n",
    "    @classmethod\n",
    "    def __keep_score(cls,score,plyr,nxt_plyr,reward):                \n",
    "        (score[plyr])[reward[0]] += 1\n",
    "        (score[nxt_plyr])[reward[1]] += 1\n",
    "        return\n",
    "\n",
    "    #\n",
    "    # Return the State, Action Key from the perspective of given player\n",
    "    #\n",
    "    @classmethod\n",
    "    def state(cls,player,board):                \n",
    "        sa = \"\"\n",
    "        sa += str(player)\n",
    "        for cell in np.reshape(board,9).tolist() : sa+= str(cell)            \n",
    "        return sa\n",
    "\n",
    "    #\n",
    "    # Run simulation to estimate Q values for state, action pairs. Random exploration policy\n",
    "    # which should be tractable with approx 6K valid board states.\n",
    "    #\n",
    "    def train_Q_values(self,num_simulations,canned_moves=None):\n",
    "        exploration = 1.0\n",
    "        decay = (1.0/num_simulations)\n",
    "        learning_rate0 = 0.05\n",
    "        learning_rate_decay = 0.1\n",
    "        discount_rate = 0.95\n",
    "        reward = 0\n",
    "        sim = 0\n",
    "        game_step = 0\n",
    "        plyr = None\n",
    "        prev_plyr = None\n",
    "        s = None\n",
    "        s1 = None\n",
    "        mv = None\n",
    "        prev_mv = None\n",
    "        prev_s = None\n",
    "        score = PlayTicTacToe.__init_score()\n",
    "            \n",
    "        while(sim < num_simulations):\n",
    "            self.__game.reset()\n",
    "            if canned_moves is None:\n",
    "                plyr = (TicTacToe.player_X,TicTacToe.player_O)[randint(0,1)] # Random player to start\n",
    "                nxt_plyr = TicTacToe.other_player(plyr)\n",
    "            mv = None\n",
    "            while(not self.__game.game_over()):\n",
    "                \n",
    "                prev_mv = mv\n",
    "                if canned_moves is None:\n",
    "                    #random.random() < (exploration-(decay*sim))):\n",
    "                    mv = self.random_move()\n",
    "                else:\n",
    "                    plyr,mv = (canned_moves[sim])[game_step]\n",
    "                    prev_plyr = TicTacToe.other_player(plyr)\n",
    "                \n",
    "                prev_s = s\n",
    "                s = PlayTicTacToe.state(plyr,self.__game.board())\n",
    "                s1 = PlayTicTacToe.state(prev_plyr,self.__game.board())\n",
    "                reward = self.__game.move(mv,plyr)\n",
    "\n",
    "                learning_rate = learning_rate0 / (1 + (sim * learning_rate_decay))\n",
    "                \n",
    "                self.add_states_if_missing(s,s1)\n",
    "\n",
    "                # Update Q Values for both players based on last play reward.\n",
    "                (self.__Q[s])[mv-1] = ((learning_rate * (self.__Q[s])[mv-1])) + ((1-learning_rate) * reward[0])\n",
    "                if(not prev_s is None):\n",
    "                    (self.__Q[prev_s])[prev_mv-1] -= (discount_rate * np.max(self.__Q[s]))\n",
    "                    (self.__Q[prev_s])[prev_mv-1] += (discount_rate * np.max(self.__Q[s1]))\n",
    "            \n",
    "                if(False):\n",
    "                    print(\"s :\" + s)\n",
    "                    print(\"s1 :\" + s1)\n",
    "                    print(\"mv :\"+ str(mv))\n",
    "                    print(\"Q[s] :\" + str(self.__Q[s]))\n",
    "                    if(not prev_s is None):\n",
    "                        print(\"prev_s :\" + prev_s)\n",
    "                        print(\"Q[prev_s] :\" + str(self.__Q[prev_s]))\n",
    "                        print(\"prev mv :\"+ str(prev_mv))\n",
    "                    print(\"\\n---------\\n\")\n",
    "                \n",
    "                if canned_moves is None:\n",
    "                    plyr = TicTacToe.other_player(plyr)\n",
    "                    prev_plyr = plyr\n",
    "                game_step += 1\n",
    "            sim += 1\n",
    "            game_step = 0\n",
    "            \n",
    "            PlayTicTacToe.__keep_score(score,plyr,prev_plyr,reward)\n",
    "            \n",
    "            if ((sim % 1000) == 0) or (sim == num_simulations) : \n",
    "                smX = \"Player X : \" + str(sim) + \" : \"\n",
    "                smO = \"Player O : \" + str(sim) + \" : \"\n",
    "                for rn,rv in TicTacToe.rewards().items():\n",
    "                    smX += rn+\" : \"+str(round(((score[TicTacToe.player_X])[rv]/sim)*100,0))+\"% \"\n",
    "                    smO += rn+\" : \"+str(round(((score[TicTacToe.player_O])[rv]/sim)*100,0))+\"% \"\n",
    "                print(smX)\n",
    "                print(smO)\n",
    "        return self.__Q\n",
    "    #\n",
    "    # Return a random action (move) that is still left\n",
    "    # to make\n",
    "    #\n",
    "    def random_move(self):\n",
    "        valid_moves = []\n",
    "        random_action = None\n",
    "        for actn in self.__game.actions():\n",
    "            if(self.__game.board()[TicTacToe.board_index(actn)] == TicTacToe.empty_cell):\n",
    "                valid_moves.append(actn)\n",
    "         \n",
    "        num_poss_moves = len(valid_moves)\n",
    "        if(num_poss_moves > 0):\n",
    "            random_action = valid_moves[randint(0, num_poss_moves-1)]\n",
    "            return random_action\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    #\n",
    "    # Given current state and lerned Q Values (if any) suggest\n",
    "    # the move that is expected to yield the highest reward.\n",
    "    #\n",
    "    def informed_move(self,st,rnd):\n",
    "        # What moves are possible at this stage\n",
    "        valid_moves = self.__game.what_are_valid_moves()\n",
    "        \n",
    "        # Are there any moves ? \n",
    "        if(np.sum(valid_moves*np.full(9,1)) == 0):\n",
    "            return None\n",
    "    \n",
    "        best_action = None\n",
    "        if(not rnd):\n",
    "            # Is there info learned for this state ?\n",
    "            informed_actions = self.Q_Vals_for_state(st)\n",
    "            if not informed_actions is None:\n",
    "                informed_actions *= valid_moves\n",
    "                best_action = np.max(informed_actions)\n",
    "                if(best_action > 0):\n",
    "                    informed_actions = np.arange(1,TicTacToe.num_actions()+1,1)[np.where(informed_actions == best_action)]\n",
    "                    best_action = informed_actions[randint(0, informed_actions.size-1)]\n",
    "                else:\n",
    "                    best_action = None\n",
    "\n",
    "        # If we found a good action then return that \n",
    "        # else pick a random action\n",
    "        if best_action == None:\n",
    "            actions = valid_moves*np.arange(1,TicTacToe.num_actions()+1,1)\n",
    "            actions = actions[np.where(actions > 0)]\n",
    "            best_action = actions[randint(0,actions.size-1)]\n",
    "\n",
    "        return int(best_action)        \n",
    "    #\n",
    "    # Play an automated game between a random player and an\n",
    "    # informed player. \n",
    "    # Return the move sequence for the entire game as s string.\n",
    "    #\n",
    "    def play(self):\n",
    "        self.__game.reset()\n",
    "        plyr = (TicTacToe.player_X,TicTacToe.player_O)[randint(0,1)] # Chose random player to start\n",
    "        mv = None\n",
    "        profile= \"\"\n",
    "        while(not self.__game.game_over()):\n",
    "            st = PlayTicTacToe.state(plyr,self.__game.board())\n",
    "            QV = self.Q_Vals_for_state(st)\n",
    "            mx = np.max(self.Q_Vals_for_state(st))\n",
    "            if(plyr == TicTacToe.player_X):\n",
    "                mv = self.informed_move(st,False) # Informed Player\n",
    "            else:\n",
    "                mv = self.informed_move(st,True) # Random Player\n",
    "            self.__game.move(mv,plyr)\n",
    "            profile += str(plyr)+\":\"+str(mv)+\"~\"\n",
    "            plyr = TicTacToe.other_player(plyr)\n",
    "        return profile\n",
    "    \n",
    "    #\n",
    "    # Add the game profile to the given game dictionary and\n",
    "    # up the count for the number of times that games was played\n",
    "    #\n",
    "    @classmethod    \n",
    "    def record_game_stats(cls,D,profile):\n",
    "        if profile in D:\n",
    "            D[profile] += 1\n",
    "        else:\n",
    "            D[profile] = 1\n",
    "        return\n",
    "    \n",
    "    def play_many(self,num):\n",
    "        informed_wins = 0\n",
    "        random_wins = 0\n",
    "        draws = 0\n",
    "        I = {}\n",
    "        R = {}\n",
    "        D = {}\n",
    "        G = {}\n",
    "        profile = \"\"\n",
    "        for x in range(0, num):\n",
    "            profile = self.play()\n",
    "            if profile not in G: G[profile]=\"\"\n",
    "            if self.__game.game_won(self.__game.board(),TicTacToe.player_X):\n",
    "                informed_wins += 1\n",
    "                PlayTicTacToe.record_game_stats(I,profile)\n",
    "            else:\n",
    "                if self.__game.game_won(self.__game.board(),TicTacToe.player_O):\n",
    "                    random_wins +=1\n",
    "                    PlayTicTacToe.record_game_stats(R,profile)\n",
    "                else: \n",
    "                    PlayTicTacToe.record_game_stats(D,profile)\n",
    "                    draws += 1\n",
    "            if(x % 100) == 0 : print (str(x))\n",
    "        print(\"Informed :\" +  str(informed_wins)+\" : \" + str(round((informed_wins/num)*100,0)))\n",
    "        print(\"Random :\" +  str(random_wins)+\" : \" + str(round((random_wins/num)*100,0)))\n",
    "        print(\"Draw :\" + str(draws)+\" : \" + str(round((draws/num)*100,0)))\n",
    "        print(\"Diff Games :\" +  str(len(G)))\n",
    "        return (I,R,D)\n",
    "    \n",
    "    #\n",
    "    # move_str is of form \"1:8~-1:1~1:6~-1:3~1:9~-1:2~\"\n",
    "    # plyr:action~.. repreat players must be alternate X,O (1,-1..)\n",
    "    # there is always a trailing ~\n",
    "    \n",
    "    #\n",
    "    # Convert a game profile string returned from play method\n",
    "    # into an array that can be passed as a canned-move to\n",
    "    # training. (Q learn)\n",
    "    #\n",
    "    @classmethod    \n",
    "    def move_str_to_array(cls,moves_as_str):\n",
    "        mvd = {}\n",
    "        mvc = 0\n",
    "        mvs = moves_as_str.split('~')\n",
    "        for mv in mvs:\n",
    "            if(len(mv)>0):\n",
    "                pl,ps = mv.split(\":\")\n",
    "                mvd[mvc]=(int(pl),int(ps))\n",
    "            mvc +=1\n",
    "        return mvd\n",
    "\n",
    "    #\n",
    "    # Convert a game profile string returned from play method\n",
    "    # into an array that can be passed as a canned-move to\n",
    "    # training. (Q learn)\n",
    "    #\n",
    "    @classmethod    \n",
    "    def move_str_to_board(cls,moves_as_str):\n",
    "        mvd = {}\n",
    "        mvc = 0\n",
    "        mvs = moves_as_str.split('~')\n",
    "        bd = np.zeros((3*3),np.int8)\n",
    "        for mv in mvs:\n",
    "            if(len(mv)>0):\n",
    "                pl,ps = mv.split(\":\")\n",
    "                bd[int(ps)-1] = int(pl)\n",
    "            mvc +=1\n",
    "        return np.reshape(bd, (3, 3))\n",
    "\n",
    "    #\n",
    "    # Convert a dictionary of game profiles returned from play_many\n",
    "    # to a dictionary of canned moves that can be passed to training (Q Learn)\n",
    "    #\n",
    "    @classmethod    \n",
    "    def moves_to_dict(cls,D):\n",
    "        MD = {}\n",
    "        i = 0\n",
    "        for mvss,cnt in D.items():\n",
    "            MD[i] = PlayTicTacToe.move_str_to_array(mvss)\n",
    "            i+=1\n",
    "        return MD\n",
    "    \n",
    "    #\n",
    "    # All possible endings. Generate moves str's for all the possible endings of the\n",
    "    # game from the perspective of the prev player. \n",
    "    #\n",
    "    # The given moves must be the moves of a valid game that played to either win/draw\n",
    "    # including the last move that won/drew the game.\n",
    "    #\n",
    "    @classmethod\n",
    "    def all_possible_endings(cls,moves_as_str,exclude_current_ending=True):\n",
    "        APM = {}\n",
    "        mvs = PlayTicTacToe.move_str_to_array(moves_as_str)\n",
    "        \n",
    "        terminal_move = mvs[len(mvs)-1] # The move that won, drew\n",
    "        last_move = mvs[len(mvs)-2] # the move we will replace with all other options\n",
    "        \n",
    "        t_plyr=terminal_move[0]\n",
    "        t_actn=terminal_move[1]\n",
    "\n",
    "        l_plyr=last_move[0]\n",
    "        l_actn=last_move[1]\n",
    "        \n",
    "        base_game = \"~\".join(mas.split(\"~\")[:-3]) # less Trailing ~ + terminal & last move\n",
    "        bd = PlayTicTacToe.move_str_to_board(base_game)\n",
    "        vmvs = TicTacToe.valid_moves(bd) \n",
    "        a=1\n",
    "        for vm in vmvs:\n",
    "            poss_end = base_game\n",
    "            if(vm):\n",
    "                if(a != t_actn): # don't include the terminal action as we will add that back on.\n",
    "                    if(not (exclude_current_ending and a == l_actn)):\n",
    "                        poss_end += \"~\"+str(l_plyr)+\":\"+str(a)\n",
    "                        poss_end += \"~\"+str(t_plyr)+\":\"+str(t_actn)+\"~\"\n",
    "                        APM[poss_end] = 0\n",
    "            a+=1            \n",
    "            \n",
    "        return(APM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QV1=QV\n",
    "#play.transfer_learning(QV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "play = PlayTicTacToe()\n",
    "play.forget_learning()\n",
    "print(play.Q_Vals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1= PlayTicTacToe.all_possible_endings(\"1:8~-1:1~1:6~-1:3~1:9~-1:2~\",False)\n",
    "t1 = {}\n",
    "t2 = {}\n",
    "t3 = {}\n",
    "t4 = {}\n",
    "t1['1:8~-1:1~1:6~-1:3~1:5~-1:2~'] = 0\n",
    "t2['1:8~-1:1~1:6~-1:3~1:7~-1:2~'] = 0\n",
    "t3['1:8~-1:1~1:6~-1:3~1:4~-1:2~'] = 0\n",
    "t4['1:8~-1:1~1:6~-1:3~1:9~-1:2~'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1057-a8a1649d6318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mQV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_Q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPlayTicTacToe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1043-fc13acf745e5>\u001b[0m in \u001b[0;36mtrain_Q_values\u001b[1;34m(self, num_simulations, canned_moves)\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mprev_s\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                     \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_mv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiscount_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m                     \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_mv\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiscount_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__Q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    QV = play.train_Q_values(len(G1),PlayTicTacToe.moves_to_dict(G1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-10-1001010\n",
      "[ 0.  0.  0. -5. -5.  0. -5.  0. -5.]\n",
      "[ 0.  0.  0. -5. -5.  0. -5.  0. -5.]\n"
     ]
    }
   ],
   "source": [
    "#qv = play.Q_Vals_for_state(\"-1-10-1001011\") + play.Q_Vals_for_state(\"1-10-1011010\") + play.Q_Vals_for_state(\"1-10-1001110\") + play.Q_Vals_for_state(\"-1-10-1001010\")\n",
    "\n",
    "qv = 0\n",
    "for key, value in play.Q_Vals().items():\n",
    "    if(np.sum((value < 0)*1)>0):\n",
    "        print(key)\n",
    "        print(value)\n",
    "        qv += value\n",
    "print(qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Informed :1 : 100.0\n",
      "Random :0 : 0.0\n",
      "Draw :0 : 0.0\n",
      "Diff Games :1\n",
      "{'1:1~-1:6~1:3~-1:4~1:5~-1:2~1:9~': 1}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "GI = {}\n",
    "GR = {}\n",
    "GD = {}\n",
    "GI,GR,GD = play.play_many(1)\n",
    "print(GI)\n",
    "print(GR)\n",
    "print(GD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testTicTacToe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1027-1e997e36153a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_to_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"test_one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msuite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestSuite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msuite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestTicTacToe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_to_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mrunner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextTestRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testTicTacToe' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Run A Single Test\n",
    "#\n",
    "test_to_run = \"test_one\"\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(testTicTacToe(test_to_run))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Run All Tests.\n",
    "#\n",
    "tests = testTicTacToe()\n",
    "suite = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNplayTicTacToe:\n",
    "    __actor_network_name = \"net/actor\"\n",
    "    __critic_network_name = \"net/critic\"\n",
    "    __x_units = 21\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # Build a four layer fully connected Network.\n",
    "    # 21 -> relu -> 21 -> relu -> 21 -> relu -> 21 -> relu\n",
    "    #\n",
    "    def construct_network(X_state,network_name):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#QV = play.train_Q_values(2000000)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ml(playg):\n",
    "    st = PlayTicTacToe.state(TicTacToe.player_X,playg.game().board())\n",
    "    QV = playg.Q_Vals_for_state(st)\n",
    "    mx = np.max(playg.Q_Vals_for_state(st))\n",
    "    print(playg.game().board())\n",
    "    print(st)\n",
    "    print(QV)\n",
    "    print((QV-mx))\n",
    "    print((QV==mx)*mx)\n",
    "    \n",
    "    #mv = self.informed_move(st,True) # Random Player\n",
    "    playg.game().move(playg.informed_move(st,False),TicTacToe.player_X)\n",
    "    print(playg.game().board())\n",
    "\n",
    "    return\n",
    "\n",
    "def play_me(playg,mv):\n",
    "    st = PlayTicTacToe.state(TicTacToe.player_X,playg.game().board())\n",
    "    QV = playg.Q_Vals_for_state(st)\n",
    "    mx = np.max(playg.Q_Vals_for_state(st))\n",
    "    print(playg.game().board())\n",
    "    playg.game().move(mv,TicTacToe.player_O)\n",
    "    print(playg.game().board())\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "play.game().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  0 -1]\n",
      " [ 0  0  1]\n",
      " [ 0  1  0]]\n",
      "1-10-1001010\n",
      "[   0.         -208.12728778    0.            1.            1.            0.\n",
      "    1.            0.            1.        ]\n",
      "[ -1.00000000e+00  -2.09127288e+02  -1.00000000e+00  -4.44089210e-16\n",
      "  -4.66293670e-15  -1.00000000e+00  -1.44328993e-15  -1.00000000e+00\n",
      "   0.00000000e+00]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "[[-1  0 -1]\n",
      " [ 0  0  1]\n",
      " [ 0  1  1]]\n"
     ]
    }
   ],
   "source": [
    "play_ml(play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  0  0]\n",
      " [ 0  0  1]\n",
      " [ 0  1  0]]\n",
      "[[-1  0 -1]\n",
      " [ 0  0  1]\n",
      " [ 0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "play_me(play,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.95833333  0.95        0.\n",
      "  0.95454545  0.          0.96153846]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(play.Q_Vals_for_state(\"1-10-1001010\"))\n",
    "print(play.Q_Vals_for_state(\"-1-10-1001010\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-10-1001010\n",
      "[ 0.  1.  0.  1.  1.  0.  1.  0.  1.]\n",
      "[ 0.          0.          0.          0.95833333  0.95        0.\n",
      "  0.95454545  0.          0.96153846]\n"
     ]
    }
   ],
   "source": [
    "st = PlayTicTacToe.state(TicTacToe.player_X,play.game().board())\n",
    "print(st)\n",
    "vms = TicTacToe.valid_moves(play.game().board())\n",
    "print(vms)\n",
    "informed_actions = play.Q_Vals_for_state(st)\n",
    "print(informed_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.         -189.12728778    0.            1.            1.            0.\n",
      "    1.            0.            1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(play.Q_Vals()[\"1-10-1001010\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-10-1001010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
